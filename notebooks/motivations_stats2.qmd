---
title: "Motivations for contributing to OS: statistical analysis"
---

# Overview

I'm redoing an earlier analysis of 6, which is about participants' reasons for contributing to open source. I was trying to incorporate all the binary response variables (yes/no to each possible motivation) in one model, but I think it just ended up being obtuse and hard to understand. I also don't feel good about using mvabund(), which did exactly what I needed, but it's an ecology tool, and basically 100% of the papers that cite it are ecology papers, so it just didn't feel right.

I'm just going to use more popular functions, and I'll use multiple small models instead of one big complicated one.

The old analysis is in notebooks/defunct/motivations_stats.qmd.

# Import packages and utilities
```{r}
project_root <- here::here() # requires that you be somewhere in the
# project directory (not above it)
# packages
suppressMessages(source(file.path(project_root, "scripts/packages.R")))
# functions and objects used across scripts
suppressMessages(source(file.path(project_root, "scripts/utils.R")))
```


# Load data
```{r}
motivations <- load_qualtrics_data("clean_data/motivations_Q6.tsv")
other_quant <- load_qualtrics_data("clean_data/other_quant.tsv")
```

# Wrangle data
```{r}
motivations_job <- cbind(motivations, other_quant$job_category)
# Rename last col
names(motivations_job)[length(names(motivations_job))] <- "job_category"
# Remove any rows where the job_category is missing
motivations_job_clean <- exclude_empty_rows(motivations_job, strict = TRUE)
# Remove rows of all 0s
motivations_job_clean <- motivations_job_clean %>%
  filter(!if_all(Job:Other, ~ .x == 0))

# drop the "Other" column
motivations_job_clean <- motivations_job_clean %>%
  select(-c("Other"))

head(motivations_job_clean)
dim(motivations_job_clean)

# This will also come in handy later.
motivation_cols <- names(motivations_job_clean)[
  -length(names(motivations_job_clean))
]
motivation_cols
```

Since other models elsewhere in this study have had trouble converging, I'm just going to combine some of the groups a priori so we can have larger sample sizes per group. This is, in a gut-sense kind of way, consistent with the power analysis I did in the earlier version of this script--the comparisons I was interested in that involved undergrads and postdocs didn't have enough statistical power for hypothesis testing. The initial group labels on the survey (faculty, postdoc, grad student, undergrad, staff researcher, non-research staff) were somewhat arbitrary, so I have no qualms about combining them now into a different set of somewhat arbitrary groups.

```{r}
combined <- motivations_job_clean %>%
  mutate(
    job_category = recode(
      job_category,
      "Post-Doc" = "Postdocs and Staff Researchers",
      "Other research staff" = "Postdocs and Staff Researchers"
    )
  )

combined <- combined %>%
  mutate(
    job_category = recode(
      job_category,
      "Grad Student" = "Students",
      "Undergraduate" = "Students"
    )
  )

head(combined)
```

# Regression on job categories

Let's make a simple logistic regression model for each motivation. The only independent variable is job_category.

```{r}
# run a separate model for each outcome (motivation)
models <- lapply(motivation_cols, function(x) {
  # wrap the column name in backticks so "My Column" becomes `My Column`
  f_text <- paste0("`", x, "`", " ~ job_category")
  f <- as.formula(f_text)
  stats::glm(f, family = "binomial", data = combined)
})

# example
models[[1]]
```


Quick AIC check just because it's easy.

```{r}
for (i in seq_along(motivation_cols)) {
  cat(
    sprintf(
      "%s  %.3f\n",
      motivation_cols[i],
      stats::AIC(models[[i]])   # AIC rounded to 3 decimals
    )
  )
}
```

Hmm. Some pretty big differences here. Improve Tools is by far the best fit. Fun and Job are a pretty poor fit.

Let's makes some null models with an intercept only, and no predictor (job_category).
```{r}
null_models <- lapply(motivation_cols, function(x) {
  # wrap the column name in backticks so "My Column" becomes `My Column`
  f_text <- paste0("`", x, "`", " ~ 1")
  f <- as.formula(f_text)
  stats::glm(f, family = "binomial", data = combined)
})
```

And let's do ANOVA to compare the null models vs. full models. (Printing an example)
```{r}
anova_results <- mapply(
  FUN = function(null_m, full_m) {
    stats::anova(null_m, full_m)
  },
  null_models,
  models,
  SIMPLIFY = FALSE
)

anova_results[[1]]
```

Let's look at p-values for all the ANOVAs.
```{r}
p_vals <- c()
for (i in seq_along(motivation_cols)) {
  p_val <- anova_results[[i]]$`Pr(>Chi)`[2]
  p_vals[i] <- p_val
  cat(
    sprintf(
      "%s  %.3f\n",
      motivation_cols[i],
      p_val # p‑value rounded to 3 decimals
    )
  )
}
```

Skills has a super low p-value, as we'd expect based on the earlier analysis with mvabund, which found a similar result. Maybe we should do a multiple test correction?

```{r}
# Choosing BH pretty arbitrarily. I don't feel a
# need to be super conservative, and I hear BH
# is more forgiving than holm.
p_fdr  <- p.adjust(p_vals, method = "BH")
for (i in seq_along(p_fdr)) {
  cat(
    sprintf(
      "%s  %.3f\n",
      motivation_cols[i],
      p_fdr[i] # p‑value rounded to 3 decimals
    )
  )
}
```

Yup. Give back is no longer significant. This too, aligns with what I saw in the previous analysis with mvabund. I didn't do quite the same procedure this time, but I noticed the coefficients for both Skills and Give back were big, and really different from the other variables, but only Skills turned out to have a significant (p-adjusted) ANOVA from univariate ANOVAs.

So, as we saw previously, Skills is the most interesting one. It's the only case where the model fit is significantly improved by inclusion of the job_category variable. It was sort of middling in terms of AIC, and I'm okay with that. Let's take a closer look at the model output.

```{r}
skills_model <- models[[which(motivation_cols=="Skills")]]
skills_model

p_fdr[[which(motivation_cols=="Skills")]]
```

Okay, apparently Faculty are our reference level. All the coefficients are positive, so everyone else is more likely to choose "Skills" than faculty.

```{r}
emm <- emmeans(skills_model, ~ job_category, type="response")
pairs(emm, type="response", infer = TRUE)  
```

Meh, not sure if these contrasts are interesting enough to be worth reporting.

# Test for trend in "skills"

In my other script, motivations_plots, we have one plot where we apparently see a trend: the probability of a respondent choosing "skills" as a motivator appears to decrease as they advance in their academic career. We will use a Cochrane-Armitage test for trend to evaluate whether this trend is real. More precisely, I believe we are evaluating whether the order "P(Yes | Undergrad) > P(Yes | Grad) > P(Yes | Postdoc) > P(Yes | Faculty)" is highly unlikely (<95% chance) given the null hypothesis that all four categories have the same probability of a "yes" response.

Full disclosure: I'm being a little p-hacky here, because I'm only trying this after I tried a series of pairwise z-tests to see whether the proportion of "yes" for "skills" was significantly different from undergrads vs. grads, grads vs. postdocs, etc. That analysis is in the old notebook. In all seriousness, I don't actually feel that I am p-hacking because I'm not just using a new test to try and make the same claim; this is a different test and we will interpret it appropriately. I'm not claiming that undergrads are more likely than grads to select skills; I'm just claiming that there is a trend across the 4 categories. 

```{r}
# Here, I haven't combined post-docs and other research staff
n_postdoc <- sum(motivations_job_clean$job_category == "Post-Doc")
n_postdoc_yes <- sum(
  motivations_job_clean$job_category == "Post-Doc" &
    motivations_job_clean$Skills == 1
)
# For the other groups, it doesn't matter if we use the raw or processed data
n_faculty <- sum(motivations_job_clean$job_category == "Faculty")
n_faculty_yes <- sum(
  motivations_job_clean$job_category == "Faculty" &
    motivations_job_clean$Skills == 1
)

n_grad <- sum(motivations_job_clean$job_category == "Grad Student")
n_grad_yes <- sum(
  motivations_job_clean$job_category == "Grad Student" &
    motivations_job_clean$Skills == 1
)

n_undergrad <- sum(motivations_job_clean$job_category == "Undergraduate")
n_undergrad_yes <- sum(
  motivations_job_clean$job_category == "Undergraduate" &
    motivations_job_clean$Skills == 1
)

n_yes <- c(
  n_undergrad_yes,
  n_grad_yes,
  n_postdoc_yes,
  n_faculty_yes
)

n_tot <- c(
  n_undergrad,
  n_grad,
  n_postdoc,
  n_faculty
)

# Assign scores 1,2,3,4 for Undergrad --> Faculty
# To indicate the ordering
scores <- 1:4

stats::prop.trend.test(
  x = n_yes,
  n = n_tot,
  score = scores
)
```

I'm honestly not sure whether this is a one-tailed or two-tailed test... I would assume one-tailed, but the documentation is terse. Anyway, even if we divide that p-value by two it's still well under p=0.05. So, yes, there is a trend of skills declining as a motivator.

# By popular demand: IT vs. Academics
Greg raised an interesting question: what about IT staff vs. academics? Let's play around with this.

I plotted the data (see motivations_plots.qmd), and it appears that these groups are somewhat different. The "Job" motivation looks to be the most different, just by eyeballing it. But let's see what the statistics say.

## Data Wrangling
```{r}
motivations_job_staff <- cbind(motivations, other_quant$job_category)
# Rename columns
names(motivations_job_staff)[length(names(
  motivations_job_staff
))] <- "job_category"
motivations_job_staff <- cbind(
  motivations_job_staff,
  other_quant$staff_categories
)
names(motivations_job_staff)[length(names(
  motivations_job_staff
))] <- "staff_category"
# Remove any rows where the job_category or staff_category are missing
motivations_job_staff_clean <- exclude_empty_rows(
  motivations_job_staff,
  strict = TRUE
)
# Remove rows of all 0s
motivations_job_staff_clean <- motivations_job_staff_clean %>%
  filter(!if_all(Job:Other, ~ .x == 0))

# drop the "Other" column
motivations_job_staff_clean <- motivations_job_staff_clean %>%
  select(-c("Other"))

head(motivations_job_staff_clean)
```

```{r}
it <- motivations_job_staff_clean %>%
  filter(staff_category == "Information Technology (IT)") %>%
  select(-c(job_category, staff_category))
it$Role <- "IT"
head(it)
dim(it)
```

```{r}
# Everyone except non-research staff
academics <- combined %>%
  filter(
    job_category == "Faculty" |
    job_category == "Students" |
    job_category == "Postdocs and Staff Researchers"
  ) %>%
    select(-job_category)
academics$Role <- "Academic"
head(academics)
dim(academics)
```

```{r}
it_acad <- rbind(it, academics)
it_acad$Role <- as.factor(it_acad$Role)
dim(it_acad)
```

Great. Now we have a data frame with the responses of IT staff and academics.

## Regression
Let's do a quick regression to see whether the IT/Academic groups improve the model fit.

```{r}
# run a separate model for each outcome (motivation)
models_it_acad <- lapply(motivation_cols, function(x) {
  # wrap the column name in backticks so "My Column" becomes `My Column`
  f_text <- paste0("`", x, "`", " ~ Role")
  f <- as.formula(f_text)
  stats::glm(f, family = "binomial", data = it_acad)
})

null_models_it_acad <- lapply(motivation_cols, function(x) {
  # wrap the column name in backticks so "My Column" becomes `My Column`
  f_text <- paste0("`", x, "`", " ~ 1")
  f <- as.formula(f_text)
  stats::glm(f, family = "binomial", data = it_acad)
})
```

If I wanted to (borderline) p-hack, I could just test "Job" and avoid multiple test correction, since the plot suggested that was the biggest difference. But I want to report robust differences that can withstand correction. So let's test them all and do the correction.

```{r}
anova_results_it_acad <- mapply(
  FUN = function(null_m, full_m) {
    stats::anova(null_m, full_m)
  },
  null_models_it_acad,
  models_it_acad,
  SIMPLIFY = FALSE
)

p_vals_it_acad <- c()
for (i in seq_along(motivation_cols)) {
  p_vals_it_acad[i] <- anova_results_it_acad[[i]]$`Pr(>Chi)`[2]
}

p_fdr_it_acad  <- p.adjust(p_vals_it_acad, method = "BH")
for (i in seq_along(p_fdr_it_acad)) {
  cat(
    sprintf(
      "%s  %.3f\n",
      motivation_cols[i],
      p_fdr_it_acad[i] # p‑value rounded to 3 decimals
    )
  )
}
```

Great. Once again, the results are in accordance with the earlier mvabund analysis. Only Job is significant. Let's look at the model.

```{r}
it_acad_job_model <- models_it_acad[[which(motivation_cols=="Job")]]
it_acad_job_model
```

I believe the negative coefficient indicates that the IT are less likely to select "yes".

And here's the full p-value from the ANOVA:

```{r}
p_fdr_it_acad[[which(motivation_cols=="Job")]]
```

```{r}
emm_it_acad <- emmeans(it_acad_job_model, ~ Role, type="response")
pairs(emm_it_acad, type="response", infer = TRUE)  
```

We can interpret that odds ratio as: The odds that an academic selects ‘Yes’ are 3.57× the odds for IT staff.

Actually, this is just the inverse of the exponentiated coefficient from the model: exp(−1.27136) = 0.280, and 1/0.280 = 3.57, which makes sense. So we didn't need emmeans. The only difference is that emmeans is just using Academic as the numerator in the odds ratio (reference level), while the model is using IT. I think integer odds are easier to understand than fractional odds (0.280), so I'll report that one. I also want to get confidence intervals. To minimize me exponentiating things by hand, I'll just redo the model with the factor level order switched.

```{r}
levels(it_acad$Role)

it_acad$Role <- relevel(it_acad$Role, ref = "IT")

rev_model <- stats::glm(Job ~ Role, family = "binomial", data = it_acad)
rev_model

exp(stats::confint(rev_model)) # exp to get it on the odds-ratio scale
```

# Last-minute add-on: NR staff vs academics
So, the previous analysis of IT vs. academics is pretty different from the rest of the analyses in this paper, in that I normally look at non-research staff, not just IT alone. If the trend still holds, I'd rather report the trend for academics vs. nr staff, for consistency with the rest of the paper. Let's see if academics are more likely to select "it's part of my job" than non-research staff.

I see two possible approaches. We could look at nr staff vs. each academic group in turn, and see if all p-values are below 0.05; OR we could lump all academics into one group, and compare the two groups, as we did for IT vs. academics. The first approach is presumably more stringent, since we'll have less power to detect subtle differences. We kind of already know that it won't be significant, since we saw above that including job category as a fixed effect didn't significantly improve model fit, when job category had 4 levels. Let's check, just to be sure.

## Approach #1: 4 job categories

```{r}
job_model <- models[[which(motivation_cols=="Job")]]
job_model

p_fdr[[which(motivation_cols=="Job")]]
```

```{r}
emm_job <- emmeans(job_model, ~ job_category, type="response")
pairs(emm_job, type="response", infer = TRUE)  
```

As expected, none of the differences between these smaller groups are significant. Let's try the larger groups (approach #1). (We'll report both results!)

## Approach #2: 2 job categories

```{r}
combined_relabeled <- combined %>%
    mutate(
    job_category = recode(
      job_category,
      "Students" = "Academic",
      "Postdocs and Staff Researchers" = "Academic",
      "Faculty" = "Academic"
    )
  )

unique(combined_relabeled$job_category)
```

Copying code from above

```{r}
# run a separate model for each outcome (motivation)
models_nr_acad <- lapply(motivation_cols, function(x) {
  # wrap the column name in backticks so "My Column" becomes `My Column`
  f_text <- paste0("`", x, "`", " ~ job_category")
  f <- as.formula(f_text)
  stats::glm(f, family = "binomial", data = combined_relabeled)
})

null_models_nr_acad <- lapply(motivation_cols, function(x) {
  # wrap the column name in backticks so "My Column" becomes `My Column`
  f_text <- paste0("`", x, "`", " ~ 1")
  f <- as.formula(f_text)
  stats::glm(f, family = "binomial", data = combined_relabeled)
})
```

If I wanted to (borderline) p-hack, I could just test "Job" and avoid multiple test correction, since the plot suggested that was the biggest difference. But I want to report robust differences that can withstand correction. So let's test them all and do the correction.

```{r}
anova_results_nr_acad <- mapply(
  FUN = function(null_m, full_m) {
    stats::anova(null_m, full_m)
  },
  null_models_nr_acad,
  models_nr_acad,
  SIMPLIFY = FALSE
)

p_vals_nr_acad <- c()
for (i in seq_along(motivation_cols)) {
  p_vals_nr_acad[i] <- anova_results_nr_acad[[i]]$`Pr(>Chi)`[2]
}

# Just to get a sense of the range pre-correction
p_vals_nr_acad

p_fdr_nr_acad  <- p.adjust(p_vals_nr_acad, method = "BH")
for (i in seq_along(p_fdr_nr_acad)) {
  cat(
    sprintf(
      "%s  %.3f\n",
      motivation_cols[i],
      p_fdr_nr_acad[i] # p‑value rounded to 3 decimals
    )
  )
}
```

Okay, so even with just two job categories, and thus bigger sample sizes, including job_category as a fixed effect does not improve the model for the "it's part of my job" question. So it's only significant for IT vs. academics, not nr staff vs. academics.

Out of curiosity, what proportion of each group selected yes?

```{r}
combined_job <- combined %>%
  select(Job, job_category)

job_summary <- combined_job %>%
  group_by(job_category) %>%
  summarise(
    n_yes = sum(Job),
    total = length(Job),
    pct_yes = round(100 * sum(Job)/length(Job), 2)
  ) %>%
    ungroup()

job_summary
```

Hm. Well, that's probably at least part of the reason why it's only significant when we group all the academics into one category. The percent of faculty who said yes is actually lower than the percent of NR staff who said yes.

We can get the same stats for just IT.

```{r}
job_summary <- rbind(
  job_summary,
  c(
    "IT",
    sum(it$Job),
    length(it$Job),
    round(sum(it$Job) / length(it$Job) * 100, 2)
  )
)

job_summary
```

Okay, so most of the discrepancy is indeed driven by IT.

Why not round it out with a row for academics, too.

```{r}
job_summary <- rbind(
  job_summary,
  c(
    "Academics",
    sum(academics$Job),
    length(academics$Job),
    round(sum(academics$Job) / length(academics$Job) * 100, 2)
  )
)

job_summary
```