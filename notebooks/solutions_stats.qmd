---
title: "Solutions Stats"
---

Some questions raised by my plots for Q10, about solutions (solutions_plots.qmd):\
- Are solution scores by job category the same for all possible pairs of job groups?\
- Are aspiring contributors significantly more likely than experienced contributors to select solutions related to learning and professional development?\
- Are experienced contributors significantly more likely than aspiring contributors to select solutions related to funding?\

# Set seed
```{r}
set.seed(42)
```

# Import packages and utilities
```{r}
project_root <- here::here() # requires that you be somewhere in the
# project directory (not above it)
# packages
suppressMessages(source(file.path(project_root, "scripts/packages.R")))
# functions and objects used across scripts
suppressMessages(source(file.path(project_root, "scripts/utils.R")))
```

# Load data
```{r}
solutions <- load_qualtrics_data("clean_data/solutions_Q10.tsv")
other_quant <- load_qualtrics_data("clean_data/other_quant.tsv")
```

# Wrangle data
First, let's add a participant ID. We'll need to keep track of these track these since observations from the same participant are not independent. We'll need to model the participants as a random effect. 
```{r}
solutions$participantID <- seq(1, nrow(solutions))
```

Next, remove empty rows, i.e. rows from respondents who didn't receive this question. As with many questions in this survey, we can cut some corners in the code because the question was mandatory. For example, no need to worry about incomplete answers.
```{r}
solutions_and_job <- solutions
solutions_and_job$job_category <- other_quant$job_category
names(solutions_and_job)[length(names(solutions_and_job))] <- "job_category"

nrow(solutions_and_job)
# from scripts/utils.R
solutions_and_job <- exclude_empty_rows(solutions_and_job, strict=TRUE) 
nrow(solutions_and_job)
```

Good. We know by now that only 233 participants saw this question.

Here's what we have so far:
```{r}
head(solutions_and_job)
```

Convert to long data, since this makes it easier to remove NAs and is necessary for the statistics.
```{r}
long_data <- solutions_and_job %>%
  pivot_longer(
    cols = -c(participantID, job_category),
    names_to = "solution",
    values_to = "utility"
  )
dim(long_data)
head(long_data)
```

Remove NAs.
```{r}
long_data <- long_data %>%
    filter(!(utility == "Non-applicable"))
dim(long_data)
```

That removed about 200 rows, out of more than 2000. So less than 10% of the responses were "non-applicable"s.

Make utility an ordered factor. Solution and job category are not inherently ordered, but we'll make them factors, and the first factor level will be the reference level for that variable. It doesn't really matter which level we use as the reference level.
```{r}
long_data$utility <- factor(
  long_data$utility,
  levels = c("Not very useful", "Useful", "Very useful"),
  ordered = TRUE
)

long_data$solution <- factor(
  long_data$solution,
  levels = unique(long_data$solution)
)

long_data$job_category <- factor(
  long_data$job_category,
  levels = unique(long_data$job_category)
)

levels(long_data$solution)
levels(long_data$job_category)
```

Ok, so it looks like our reference levels are computing environments and faculty. That's fine. It doesn't really matter.

# Create candidate models
I'd like to fit a cumulative-logit mixed model, a.k.a. an ordinal regression model, using the `clmm` function from the ordinal package. (I am not using `polr` from the MASS package because it does not allow random effects.) I know we want to include participantID as a random effect, but I'm not really sure how to model solution. I think it would be best to compare different models.

Note that the next few cells take several minutes to run.

## Model 1: job_category * solution interaction
Here, I'm modeling job_category and solution as independent fixed effects, and assuming that there is also an effect from the interaction of the two. This way, we get a global slope for job_category, a global slope for solution, a global slope for the interaction (I think), and a global intercept. Adding participant as a random effect allows each participant to have their own deviation from the global intercept.
```{r}
fit1 <- ordinal::clmm(utility ~ job_category * solution +      
              (1 | participantID),                 
            data = long_data, link = "logit", Hess = TRUE)
```
Hm. I get a warning that "Hessian is numerically singular: parameters are not uniquely determined" and "Absolute convergence criterion was met, but relative criterion was not met". The internet suggests that this might mean that some job-category × solution combinations have few or zero responses in one of the utility levels, so the full job_category * solution interaction is over-parameterised.

## Model 2: solution as a random effect, no correlation between participant intercept and job effect
Here's another formulation. In this case, solution is another random effect, so we only get one global slope from job_category, but each solution intercept (as well as each participant intercept) is allowed to deviate from the global intercept. We assume that across solutions, the deviations in job_cateogry effect from the global effect of job_category are not correlated with that solution's intercept's deviation from the global intercept.
```{r}
fit2 <- ordinal::clmm(utility ~ job_category +       
              (1 | solution) +
              (1 | participantID) +
              (0 + job_category | solution),                
            data = long_data, link = "logit", Hess = TRUE)
```

Next, we again have 4 terms, like we did in the first model: a global intercept, slopes for job_category and solution, and a slope for the interaction. Now, we also estimate the deviance of each of these terms from the global baseline for each participant, and we also estimate the correlations between the deviations for each possible combination of the 4 terms, for each participant. Er, I think. (Helpful cheat sheet: https://stats.stackexchange.com/questions/13166/rs-lmer-cheat-sheet)

This one measures a ton of parameters... ABANDONED; NEVER CONVERGED
```{r}
# fit3 <- ordinal::clmm(utility ~ job_category * solution +       # fixed effects
#               (0 + job_category*solution | participantID),
#             data = long_data, link = "logit", Hess = TRUE)
```

All the models seem to be struggling a bit. Let's explore the data for a moment.
```{r}
# three way cross tabs (xtabs) and flatten the table
# code from: https://ladal.edu.au/tutorials/regression/regression.html
ftable(xtabs(~ job_category + solution + utility, data = long_data))
```

Hm. Indeed, the data are sparse in places, particularly for undergraduates. Perhaps we should combine postdocs + staff researchers, as well as undergrads + grad students.
```{r}
combined <- long_data %>%
  mutate(
    job_category = recode(
      job_category,
      "Post-Doc" = "Postdocs and Staff Researchers",
      "Other research staff" = "Postdocs and Staff Researchers"
    )
  )

combined <- combined %>%
  mutate(
    job_category = recode(
      job_category,
      "Grad Student" = "Student",
      "Undergraduate" = "Student"
    )
  )
```

Now let's run models 1 and 2 again, but with this consolidated dataset.

## Model 1b: Model 1, but with consolidated data
```{r}
fit1b <- ordinal::clmm(utility ~ job_category * solution +      
              (1 | participantID),                 
            data = combined, link = "logit", Hess = TRUE)
```

No warning this time, and I feel like it finished faster. My hunch is that this re-labeled dataset will lead to better results.

## Model 2b: Model 2, but with consolidated data
```{r}
fit2b <- ordinal::clmm(utility ~ job_category +       
              (1 | solution) +
              (1 | participantID) +
              (0 + job_category | solution),                
            data = combined, link = "logit", Hess = TRUE)
```

So, those are two fairly complex models that I think capture the important variation. Let's compare them to some simpler models.

## Model 3: No job category
Let's make a null model where job category doesn't matter. (Using the consolidated data)
```{r}
fit3 <- ordinal::clmm(utility ~ solution +      
              (1 | participantID),                 
            data = combined, link = "logit", Hess = TRUE)
```

## Model 4: No solution category
How about a model where solution doesn't matter?
```{r}
fit4 <- ordinal::clmm(utility ~ job_category +      
              (1 | participantID),                 
            data = combined, link = "logit", Hess = TRUE)
```

## Model 5: job_category + solution
In this minimal model, we include job_category + solution, but without any interaction. This model says that we can predict the rating by simply adding the effect of job category and the effect of solution, with no additional effect from combining a particular job category with a particular solution.
```{r}
fit5 <- ordinal::clmm(utility ~ job_category + solution +
              (1 | participantID),                 
            data = combined, link = "logit", Hess = TRUE)
```

## Model 6: no random effects
Do we really need to account for participants' individual baselines?
```{r}
# note clm function bc clmm is for mixed models
fit6 <- ordinal::clm(utility ~ job_category * solution,                 
            data = combined, link = "logit", Hess = TRUE)
```

# Compare models
```{r}
models <- list(
  "fit1"=fit1, # job_category * solution, sparser data
  "fit2"=fit2, # solution as random effect, sparser data
  "fit1b"=fit1b, # job_category * solution, denser data
  "fit2b"=fit2b, # solution as random effect, denser data
  "fit3"=fit3, # Null model: no job
  "fit4"=fit4, # Null model: no solution
  "fit5"=fit5, # Null model: no interaction
  "fit6"=fit6 # Null model: no participants
)
```

First, let's get a general sense of goodness-of-fit by looking at the AICs. You're not supposed to compare AICs for models fit to different data sets (models 1 and 2 are using the sparser data), but since I've only changed the job_category labels, but not the observations or the number of observations, I think this is ok.
```{r}
sapply(models, function(x) round(stats::AIC(x)))
```

The AICs for all the models are fairly similar, except in two cases: #4, where solution isn't doesn't matter, and job_category alone influences the response, and #6, where participant ID doesn't matter. Both of these make sense. Model 5, where job category and solution have no interaction, does fairly well. Maybe job-solution interactions are subtle.

Model 1b looks the best. According to the internet, a delta AIC of more than ten is pretty substantial, and here we have a difference of 20 between the best and second-best.

Let's check the condition number of the Hessian. I don't really understand what this is, but the clmm2 tutorial says that high numbers, say larger than say 10^4 or 10^6, indicate poor fit.
```{r}
sapply(models, function(x) 
summary(x)$info["cond.H"]
)
```
Okay, depending on my random seed, fit1 either gives a NaN or a high value here. All the other models look decent.

## Complex models vs null models

Let's use an anova to compare the two models that scored the best in terms of AIC. Since they also happen to be nested, an anova works here.
```{r}
stats::anova(fit1b, fit5)
```

That's a significant p-value. It looks like the interaction term is worth including.

Let's also double-check that participants are worth including.
```{r}
stats::anova(fit1b, fit6)
```

Yep, definitely want to include those.

Does it matter whether we include job as a variable? Let's compare it to the model with job + solution, without an interaction term.
```{r}
stats::anova(fit3, fit5)
```

It appears that job is also significant in explaining the variation in the data.

## More goodness of fit evaluation

How else to evaluate the models? The `ordinal` package provides goodness-of-fit functions nominal_test and scale_test, but as far as I can tell, these only work on clm objects, not clmm objects. (mixed models)

Model 2b had a similar AIC as model 5. While I can't compare model 1b and model 2b with anova, since they're not nested, I can at least glance at the standard errors of the coefficients, which give me a sense of the precision of the coefficient estimates.
```{r}
summary(fit1b$coefficients)
summary(fit2b$coefficients)
```

Hm. So fit1b had the lowest AIC of all the models and is significantly better at explaining the variation than the equivalent minimal model without an interaction term. However, the coefficients of fit2b have smaller SEs than those of fit1b. 

How about the log likelihoods?
```{r}
LL <- sapply(models, function(x) x$logLik)
# These are a bit hard to read to I am reordering them
LL[order(LL)]
```
In this case, surprisingly, fit1 looks best. But only #4 and #6 look really bad. The rest are sort of close together, and fit1b is second best, not too far behind fit1.

So, I find myself in the annoying situation of having several g-o-f tests that don't quite agree. However, I think I'll go with fit1b. It had the best AIC and the second-best log-likelihood. The SEs are a little concerning, but I don't think the SEs are a super reliable indicator of g-o-f anyway. This model consistently had pretty good g-o-f metrics, and I think it also intuitively makes the most sense.  

# Interpreting the model results
```{r}
summary(fit1b)
```

This is a lot to interpret. I'll do my best. First, let's just at the main effects, i.e. the effects of job category and solution. In the summary above, each job category is compared to Faculty, our job reference level, for the solution Computing environments, our solution reference level. The "Estimate" for job_categoryStudent is 1.66906, which indicates students have odds of e^1.67=5.3 of rating that solution at least one category higher than faculty. 

The solution Publicity has a coefficient of -0.66811, indicating that faculty have odds of e^0.67=2 of rating Publicity one level lower than Computing Environments.

The interactions, e.g. job_categoryPostdocs and Staff Researchers:solutionPublicity, indicate extra log-odds only for that specific job × solution pair beyond the two main effects. So in that example, postdocs and staff researchers have an extra log-odds of 0.78228 (odds of e^0.78228=2.186) of giving publicity a higher rating than computing environments, as compared to faculty.

Interestingly, none of our p-values are super significant for interactions, meaning none of the interactions are really significant on their own. The most significant effects (three asterisks) were all solutions: A learning community (-), Event planning (-), Mentoring programs (-), Education (-), Sustainability grants (+).

So, faculty had significantly higher odds of selecting sustainability grants than computing environments; significantly lower odds of selections education, mentoring, etc. than computing environments.

One job category did get two asterisks:\
Coefficients:\
                                                                         Estimate Std. Error z value Pr(>|z|) \
job_categoryStudent                                                       1.66906    0.61824   2.700 0.006941 ** \

So, students had somewhat significantly higher odds of selecting computing environments than faculty.

So, painting this with a really broad brush, we might say that responses vary across solutions more than they vary across job categories, at least in the sense that there are more significant differences within faculty than between faculty vs. students.

Since coefficients are hard to interpret, let's get contrasts using the emmeans package. The contrast essentially indicates the difference between two factor's effect sizes. So instead of comparing the coefficients by eye, we can just calculate contrasts that tell us how big the difference is, for each pair of coefficients.

## Estimated marginal means
So, here's my attempt to make sense of a complicated post-hoc exploration of a complicated model. Ordinal regression with the ordinal package--and ordinal regression in general, I think--assumes that there is a continuous random variable--a "Latent" variable--underlying the categorical outcomes. The category boundaries are then thresholds on the continuous function. The emmeans package is a package for getting estimated marginal means from your model: mean outcomes for certain variables while holding other variables constant. The emmeans function can be run in various modes that will change the reported means from the default "latent" scale (whose bounds are arbitrary) to something else. mode = "prob" will report descriptive statistics on the probability distribution of each rating. mode = "mean.class" will report the means of these distributions as probabilities on a scale of 1 to n, where n is the number of outcome categories in your data set. So if you have three outcomes, e.g. not very useful, useful, very useful, and you obtain an average rating of 2.1 for a particular solution with mode="mean.class", this means that the average rating for that solution was a teensy bit above "useful". 

```{r}
# A warning to keep in mind when using mode="prob",
# and I assume it also applies to mode="mean.class":
# https://stats.stackexchange.com/questions/615711/why-are-emmip-response-y-axis-numbers-not-probabilities-for-ordinal-regressi#:~:text=There%20are%20several%20ways%20to,emmeans()%20or%20ref_grid()%20.&text=With%20mode%20=%20%22latent%22%20%2C,corresponding%20vector%20of%20coefficient%20estimates.
emm <- emmeans(fit1b, ~ job_category * solution, mode = "mean.class")
by_job <- summary(
  pairs(emm, by = "job_category"),
  infer = TRUE # infer CIs
)

by_job
```

Let's glance at the significant differences.
```{r}
sig_by_job <- subset(by_job, p.value < 0.05)
sig_by_job
```

Okay, so here, the "estimate" column shows the difference in estimated marginal means for the two levels of interest, holding the other factor level constant (of my two factors, job and solution). So when the contrast is Computing environments vs. A learning community, the job_category is Faculty, and the estimate is 0.57, this indicates that the difference between the estimates of faculty's average rating of computer environments and their average rating of a learning community is 0.56, on a three-point scale.

```{r}
subset(summary(emm), job_category=="Faculty" & solution == "Computing environments")$mean.class - subset(summary(emm), job_category=="Faculty" & solution == "A learning community")$mean.class
```

# Plotting the data (Unfinished)

I'd like to plot these data as a heat map. However, I find it really confusing to rely on the sign of the estimate (+/-) to tell me which solution is preferred. I'd rather have only positive values, and instead of using the sign to indicate which solution is preferred, we'll use the order of the solutions to indicate which solution is preferred. Let's say that the solution on the y-axis is always the preferred solution, so that the color/value merely indicates the extent to which respondents prefer the solution on the y-axis. 

To keep things simple for now, let's just look at non-research staff.

```{r}
nr <- subset(by_job, job_category == "Non-research Staff")
nr$contrast <- as.character(nr$contrast)
soln1 <- unname(sapply(nr$contrast, function(x) strsplit(x, " - ")[[1]][1]))
soln2 <- unname(sapply(nr$contrast, function(x) strsplit(x, " - ")[[1]][2]))
nr2 <- data.frame(
  soln1 = soln1,
  soln2 = soln2,
  value = nr$estimate,
  pval = nr$p.value,
  significant = ifelse(nr$p.value < 0.05, "*", "")
)
head(nr2)
```

```{r}
nr2_clean <- nr2 %>%
  # if value < 0 swap soln1/soln2 and flip the sign--one step at a time
  mutate(
    soln_pref = if_else(value < 0, soln2, soln1),   # preferred solution
    soln_other = if_else(value < 0, soln1, soln2),  # the other solution
    value_pos  = if_else(value < 0, -value, value)  # positive magnitude
  ) %>% 
  
  # keep only the modified columns
  select(
    soln1 = soln_pref,
    soln2 = soln_other,
    value = value_pos,
    significant
  )

#Check the before and after
tail(nr2)
tail(nr2_clean)
```

In this new data frame, soln1 is the preferred solution.

Let's reorder the factor levels and make them the same for both solution columns. This ensures that all solutions appear on both axes, in the same order.

```{r}
sol_levels <- sort(unique(c(nr2_clean$soln1, nr2_clean$soln2)))

plot_df <- nr2_clean %>% 
  mutate(
    soln1 = factor(soln1, levels = sol_levels),
    soln2 = factor(soln2, levels = sol_levels)
  ) %>% 
  # complete fills in missing combinations
  # as new rows with NA values
  complete(soln1, soln2)

plot_df
```

```{r, fig.width=12, fig.height=10}
nr_heatmap <- ggplot(
  data = plot_df,
  aes(x = soln2, y = soln1, fill = value)
) +
  geom_tile(width = 0.97, height = 0.97) +   # gaps for the grid to show
  geom_text(aes(label = significant), color = "black", size = 8) +
  # Reverse x‑axis order and put it on top
  scale_x_discrete(limits = rev(sol_levels), position = "top") +
  scale_fill_gradient(low = "white", high = "darkred", na.value = "#e6e6e6") +
  ggtitle("Solutions preferred by non-research staff") +
  labs(
    fill = "Extent to which\ny-axis solution is\npreferred (difference in\nestimated mean rating)"
  ) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_text(
      angle = 46,
      vjust = 0,
      hjust = 0,
      size = 20
    ),
    axis.text.y = element_text(size = 20),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_blank(),
    legend.title = element_text(size = 15),
    panel.background = element_blank(),
    panel.grid = element_line(linetype = "solid", color = "gray90"),
    plot.title = element_text(
      hjust = 0.5,
      size = 24,
      margin = margin(b = 15)
    ),
    plot.margin = unit(c(0.3, 0.3, 0.3, 0.3), "cm")
  )

#nr_heatmap
```

Presumably, those warnings are just because our data frame contains a bunch of NA values, for combinations where the x-axis solution is preferred, and so we have no data.
```{r}
save_plot("solns_heatmap_nrstaff.tiff", 12, 10, p=nr_heatmap)
```

I'd like to "validate" this result by eyeballing a plot of the number of "Not very useful", "Useful", and "Very useful" results for non-research staff for a couple of solutions, say, sustainability grants and a learning community. The "favorite" solution question suggested that a learning community was more popular than grants among research staff, so the result here, which suggests grants are the most popular solution, is somewhat surprising.

```{r}
t <- long_data %>%
  filter(job_category == "Non-research Staff") %>%
  filter(solution == "Computing environments" |
        solution == "A learning community" |
        solution == "Sustainability grants" |
        solution == "Publicity")

```

```{r, fig.width=12, fig.height=10}
gbc <- grouped_bar_chart(
  df = t,
  x_var = "solution",
  fill_var = "utility",
  title = "Non-research staff solution preferences")

gbc
```

```{r}
save_plot("solns_sanity_check_nrstaff.tiff", 12, 10, p=gbc)
```

This looks reasonably consistent with the model. From this bar chart, only Sustainability grants and Computing environments have a strong, linear slope with "very useful" being the most common response; and of the two, that trend is more pronounced for Sustainability grants. Previously, I found that "A learning community" was the number 1 solution from non-research staff when we asked them to choose their top solution (see solutions_plots.qmd), but apparently when they were allowed to rate solutions on a Likert scale, Sustainability grants was more popular, basically.

This plot is pretty complicated and convoluted. If I do want to publish it, I'll probably want to use a diverging color scheme for positive and negative differences, and keep only squares on the diagonal gray. 

# Global EMMs

Can we say which solutions were generally popular overall? Here we get the expected rating for each solution, averaging over levels of job_category and participants. By default, each job contributes in proportion to its sample size. The mean.class value indicates the estimated rating, which is the sum of the probabilities of each of the likert levels.
```{r}
global_soln_ratings <- emmeans(fit1b, ~ solution, mode = "mean.class", weights="cells")
```
TODO: What's up with that warning?
Need to figure out weighting strategy

```{r}
global_sum <- summary(global_soln_ratings)
global_sum <- global_sum %>% arrange(desc(mean.class))
global_sum
```

Hmm. That's not quite what I expected. "A learning community" did well when we looked at the total "points" earned from the Likert scale question (see solutions_plots.qmd), and from the "choose your favorite" question, but here it didn't do so well. Maybe that's because we are ignoring job as a factor? But we were also ignoring it with our simple descriptive statistics... Hm.

Let's make some more heat maps to continue getting a sense of the data.
TODO: Plot global solution ratings, and ratings for each job
```{r}
pairs(global_soln_ratings)
```













```{r}
job_categories <- unique(long_data$job_category)
by_job_dfs <- sapply(job_categories, function(x){
  df <- subset(by_job, job_category == x)
  df$contrast <- as.character(nr$contrast)
})
nr <- subset(by_job, job_category == "Non-research Staff")
nr$contrast <- as.character(nr$contrast)
soln1 <- unname(sapply(nr$contrast, function(x) strsplit(x, " - ")[[1]][1]))
soln2 <- unname(sapply(nr$contrast, function(x) strsplit(x, " - ")[[1]][2]))
nr2 <- data.frame(
  soln1 = soln1,
  soln2 = soln2,
  value = nr$estimate,
  pval = nr$p.value,
  significant = ifelse(nr$p.value < 0.05, "*", "")
)
```










```{r}
emm_soln <- emmeans(fit1b, ~ job_category * solution)
by_soln <- summary(
  pairs(emm_soln, by = "solution"),
  infer = TRUE # infer CIs
) 

by_soln
```