---
title: "Solutions Stats"
---

# Overview

N.B.: This notebook takes a relatively long time to publish with Quarto.

Some questions raised by my plots for Q10, about solutions (solutions_plots.qmd):\
- Are solution scores by job category the same for all possible pairs of job groups?\
- Are non-research staff significantly more likely than other groups to want a learning community?\
- Are aspiring contributors significantly more likely than experienced contributors to select solutions related to learning and professional development?\
- Are experienced contributors significantly more likely than aspiring contributors to select solutions related to funding?\

# Set seed
```{r}
set.seed(42)
```

# Import packages and utilities
```{r}
project_root <- here::here() # requires that you be somewhere in the
# project directory (not above it)
# packages
suppressMessages(source(file.path(project_root, "scripts/packages.R")))
# functions and objects used across scripts
suppressMessages(source(file.path(project_root, "scripts/utils.R")))
```

# Load data
```{r}
solutions <- load_qualtrics_data("clean_data/solutions_Q10.tsv")
other_quant <- load_qualtrics_data("clean_data/other_quant.tsv")
```

# Wrangle data
First, let's add a participant ID. We'll need to keep track of these track these since observations from the same participant are not independent. We'll need to model the participants as a random effect. 
```{r}
solutions$participantID <- seq(1, nrow(solutions))
```

Next, remove empty rows, i.e. rows from respondents who didn't receive this question. As with many questions in this survey, we can cut some corners in the code because the question was mandatory. For example, no need to worry about incomplete answers.
```{r}
solutions_and_job <- solutions
solutions_and_job$job_category <- other_quant$job_category
names(solutions_and_job)[length(names(solutions_and_job))] <- "job_category"

nrow(solutions_and_job)
# from scripts/utils.R
solutions_and_job <- exclude_empty_rows(solutions_and_job, strict=TRUE) 
nrow(solutions_and_job)
```

Good. We know by now that only 233 participants saw this question.

Here's what we have so far:
```{r}
head(solutions_and_job)
```

Convert to long data, since this makes it easier to remove NAs and is necessary for the statistics.
```{r}
long_data <- solutions_and_job %>%
  pivot_longer(
    cols = -c(participantID, job_category),
    names_to = "solution",
    values_to = "utility"
  )
dim(long_data)
head(long_data)
```

Remove NAs.
```{r}
long_data <- long_data %>%
    filter(!(utility == "Non-applicable"))
dim(long_data)
```

That removed about 200 rows, out of more than 2000. So less than 10% of the responses were "non-applicable"s.

Make utility an ordered factor. Solution and job category are not inherently ordered, but we'll make them factors, and the first factor level will be the reference level for that variable. It doesn't really matter which level we use as the reference level.
```{r}
long_data$utility <- factor(
  long_data$utility,
  levels = c("Not very useful", "Useful", "Very useful"),
  ordered = TRUE
)

long_data$solution <- factor(
  long_data$solution,
  levels = unique(long_data$solution)
)

long_data$job_category <- factor(
  long_data$job_category,
  levels = unique(long_data$job_category)
)

levels(long_data$solution)
levels(long_data$job_category)
```

Ok, so it looks like our reference levels are computing environments and faculty. That's fine. It doesn't really matter.

# Create candidate models
I'd like to fit a cumulative-logit mixed model, a.k.a. an ordinal regression model, using the `clmm` function from the ordinal package. (I am not using `polr` from the MASS package because it does not allow random effects.) I know we want to include participantID as a random effect, but I'm not really sure how to model solution. I think it would be best to compare different models.

Note that the next few cells take several minutes to run.

## Model 1: job_category * solution interaction
Here, I'm modeling job_category and solution as independent fixed effects, and assuming that there is also an effect from the interaction of the two. This way, we get a global slope for job_category, a global slope for solution, a global slope for the interaction (I think), and a global intercept. Adding participant as a random effect allows each participant to have their own deviation from the global intercept.
```{r}
fit1 <- ordinal::clmm(utility ~ job_category * solution +      
              (1 | participantID),                 
            data = long_data, link = "logit", Hess = TRUE)
```
Hm. I get a warning that "Hessian is numerically singular: parameters are not uniquely determined" and "Absolute convergence criterion was met, but relative criterion was not met". The internet suggests that this might mean that some job-category Ã— solution combinations have few or zero responses in one of the utility levels, so the full job_category * solution interaction is over-parameterised.

## Model 2: solution as a random effect, no correlation between participant intercept and job effect
Here's another formulation. In this case, solution is another random effect, so we only get one global slope from job_category, but each solution intercept (as well as each participant intercept) is allowed to deviate from the global intercept. We assume that across solutions, the deviations in job_cateogry effect from the global effect of job_category are not correlated with that solution's intercept's deviation from the global intercept.
```{r}
fit2 <- ordinal::clmm(utility ~ job_category +       
              (1 | solution) +
              (1 | participantID) +
              (0 + job_category | solution),                
            data = long_data, link = "logit", Hess = TRUE)
```

Next, we again have 4 terms, like we did in the first model: a global intercept, slopes for job_category and solution, and a slope for the interaction. Now, we also estimate the deviance of each of these terms from the global baseline for each participant, and we also estimate the correlations between the deviations for each possible combination of the 4 terms, for each participant. Er, I think. (Helpful cheat sheet: https://stats.stackexchange.com/questions/13166/rs-lmer-cheat-sheet)

This one measures a ton of parameters... ABANDONED; NEVER CONVERGED
```{r}
# fit3 <- ordinal::clmm(utility ~ job_category * solution +       # fixed effects
#               (0 + job_category*solution | participantID),
#             data = long_data, link = "logit", Hess = TRUE)
```

All the models seem to be struggling a bit. Let's explore the data for a moment.
```{r}
# three way cross tabs (xtabs) and flatten the table
# code from: https://ladal.edu.au/tutorials/regression/regression.html
ftable(xtabs(~ job_category + solution + utility, data = long_data))
```

Hm. Indeed, the data are sparse in places, particularly for undergraduates. Perhaps we should combine postdocs + staff researchers, as well as undergrads + grad students.
```{r}
combined <- long_data %>%
  mutate(
    job_category = recode(
      job_category,
      "Post-Doc" = "Postdocs and Staff Researchers",
      "Other research staff" = "Postdocs and Staff Researchers"
    )
  )

combined <- combined %>%
  mutate(
    job_category = recode(
      job_category,
      "Grad Student" = "Students",
      "Undergraduate" = "Students"
    )
  )
```

Now let's run models 1 and 2 again, but with this consolidated dataset.

## Model 1b: Model 1, but with consolidated data
```{r}
fit1b <- ordinal::clmm(utility ~ job_category * solution +      
              (1 | participantID),                 
            data = combined, link = "logit", Hess = TRUE)
```

No warning this time, and I feel like it finished faster. My hunch is that this re-labeled dataset will lead to better results.

## Model 2b: Model 2, but with consolidated data
```{r}
fit2b <- ordinal::clmm(utility ~ job_category +       
              (1 | solution) +
              (1 | participantID) +
              (0 + job_category | solution),                
            data = combined, link = "logit", Hess = TRUE)
```

So, those are two fairly complex models that I think capture the important variation. Let's compare them to some simpler models.

## Model 3: No job category
Let's make a null model where job category doesn't matter. (Using the consolidated data)
```{r}
fit3 <- ordinal::clmm(utility ~ solution +      
              (1 | participantID),                 
            data = combined, link = "logit", Hess = TRUE)
```

## Model 4: No solution category
How about a model where solution doesn't matter?
```{r}
fit4 <- ordinal::clmm(utility ~ job_category +      
              (1 | participantID),                 
            data = combined, link = "logit", Hess = TRUE)
```

## Model 5: job_category + solution
In this minimal model, we include job_category + solution, but without any interaction. This model says that we can predict the rating by simply adding the effect of job category and the effect of solution, with no additional effect from combining a particular job category with a particular solution.
```{r}
fit5 <- ordinal::clmm(utility ~ job_category + solution +
              (1 | participantID),                 
            data = combined, link = "logit", Hess = TRUE)
```

## Model 6: no random effects
Do we really need to account for participants' individual baselines?
```{r}
# note clm function bc clmm is for mixed models
fit6 <- ordinal::clm(utility ~ job_category * solution,                 
            data = combined, link = "logit", Hess = TRUE)
```

# Compare models
```{r}
models <- list(
  "fit1"=fit1, # job_category * solution, sparser data
  "fit2"=fit2, # solution as random effect, sparser data
  "fit1b"=fit1b, # job_category * solution, denser data
  "fit2b"=fit2b, # solution as random effect, denser data
  "fit3"=fit3, # Null model: no job
  "fit4"=fit4, # Null model: no solution
  "fit5"=fit5, # Null model: no interaction
  "fit6"=fit6 # Null model: no participants
)
```

First, let's get a general sense of goodness-of-fit by looking at the AICs. You're not supposed to compare AICs for models fit to different data sets (models 1 and 2 are using the sparser data), but since I've only changed the job_category labels, but not the observations or the number of observations, I think this is ok.
```{r}
sapply(models, function(x) round(stats::AIC(x)))
```

The AICs for all the models are fairly similar, except in two cases: #4, where solution isn't doesn't matter, and job_category alone influences the response, and #6, where participant ID doesn't matter. Both of these make sense. Model 5, where job category and solution have no interaction, does fairly well. Maybe job-solution interactions are subtle.

Model 1b looks the best. According to the internet, a delta AIC of more than ten is pretty substantial, and here we have a difference of 20 between the best and second-best.

Let's check the condition number of the Hessian. I don't really understand what this is, but the clmm2 tutorial says that high numbers, say larger than say 10^4 or 10^6, indicate poor fit.
```{r}
sapply(models, function(x) 
summary(x)$info["cond.H"]
)
```
Okay, depending on my random seed, fit1 either gives a NaN or a high value here. All the other models look decent.

## Complex models vs null models

Let's use an anova to compare the two models that scored the best in terms of AIC. Since they also happen to be nested, an anova works here.
```{r}
stats::anova(fit1b, fit5)
```

That's a significant p-value. It looks like the interaction term is worth including.

Let's also double-check that participants are worth including.
```{r}
stats::anova(fit1b, fit6)
```

Yep, definitely want to include those.

Does it matter whether we include job as a variable? Let's compare it to the model with job + solution, without an interaction term.
```{r}
stats::anova(fit3, fit5)
```

It appears that job is also significant in explaining the variation in the data.

## More goodness of fit evaluation

How else to evaluate the models? The `ordinal` package provides goodness-of-fit functions nominal_test and scale_test, but these only work on clm objects, not clmm objects (mixed models).

Model 2b had a similar AIC as model 5. While I can't compare model 1b and model 2b with anova, since they're not nested, I can at least glance at the standard errors of the coefficients, which I think gives me a sense of the precision of the coefficient estimates.
```{r}
summary(fit1b$coefficients)
summary(fit2b$coefficients)
```

Hm. So fit1b had the lowest AIC of all the models and is significantly better at explaining the variation than the equivalent minimal model without an interaction term. However, the coefficients of fit2b have smaller SEs than those of fit1b. 

How about the log likelihoods?
```{r}
LL <- sapply(models, function(x) x$logLik)
# These are a bit hard to read so I am reordering them
LL[order(LL)]
```
In this case, surprisingly, fit1 looks best. But according to the interwebs, this can happen just from having more parameters. So I think we should probably only use this to compare models that have the same number of parameters, e.g. fit3 vs. fit4.

So, I find myself in the annoying situation of having several g-o-f tests that don't perfectly agree. However, I'm leaning toward fit1b. It had the best AIC and the second-best log-likelihood. The SEs are a little concerning, but I don't think the SEs are a super reliable indicator of g-o-f anyway(?).  This model consistently had pretty good g-o-f metrics, and I think it also intuitively makes the most sense.

Let's do one more test. fit6 is the equivalent model to fit1b, but with fixed effects only. Since we can do the nominal_test and scale_test on this model, let's try it and see if it sets off any red flags.

```{r}
nominal_test(fit6)
scale_test(fit6)
```

Ouch. That's not ideal. Maybe we can proceed with caution, and follow up with a non-parametric test on whatever trends we find? https://www.fharrell.com/post/po/

# Interpreting the model results
```{r}
summary(fit1b)
```

This is a lot to interpret. I'll do my best. First, let's just at the main effects, i.e. the effects of job category and solution. In the summary above, each job category is compared to Faculty, our job reference level, for the solution Computing environments, our solution reference level. The "Estimate" for job_categoryStudents is 1.66906, which indicates students have odds of e^1.67=5.3 of rating that solution at least one category higher than faculty. 

The solution Publicity has a coefficient of -0.66811, indicating that faculty have odds of e^0.67=2 of rating Publicity one level lower than Computing Environments.

The interactions, e.g. job_categoryPostdocs and Staff Researchers:solutionPublicity, indicate extra log-odds only for that specific job Ã— solution pair beyond the two main effects. So in that example, postdocs and staff researchers have an extra log-odds of 0.78228 (odds of e^0.78228=2.186) of giving publicity a higher rating than computing environments, as compared to faculty.

Interestingly, none of our p-values are super significant for interactions, meaning none of the interactions are really significant on their own. The most significant effects (three asterisks) were all solutions: A learning community (-), Event planning (-), Mentoring programs (-), Education (-), Sustainability grants (+).

So, faculty had significantly higher odds of selecting sustainability grants than computing environments; significantly lower odds of selections education, mentoring, etc. than computing environments.

One job category did get two asterisks:\
Coefficients:\
                                                                         Estimate Std. Error z value Pr(>|z|) \
job_categoryStudents                                                       1.66906    0.61824   2.700 0.006941 ** \

So, students had somewhat significantly higher odds of selecting computing environments than faculty.

So, painting this with a really broad brush, we might say that responses vary across solutions more than they vary across job categories, at least in the sense that there are more significant differences within faculty than between faculty vs. students.

Since coefficients are hard to interpret, let's get contrasts using the emmeans package. The contrast essentially indicates the difference between two factors' effect sizes. So instead of comparing the coefficients by eye, we can just calculate contrasts that tell us how big the difference is, for each pair of coefficients.

# Estimated marginal means
So, here's my attempt to make sense of a complicated post-hoc exploration of a complicated model. Ordinal regression with the `ordinal` package--and ordinal regression in general, I think--assumes that there is a continuous random variable--a "latent" variable--underlying the categorical outcomes. The category boundaries are then thresholds on the continuous function. The emmeans package gets estimated marginal means from your model: mean outcomes for certain variables while holding other variables constant. The emmeans function can be run in various modes that will change the reported means from the default "latent" scale (whose bounds are arbitrary) to something else. mode = "prob" will report descriptive statistics on the probability distribution of each rating. mode = "mean.class" will report the means of these distributions as probabilities on a scale of 1 to n, where n is the number of outcome categories in your data set. So if you have three outcomes, e.g. not very useful, useful, very useful, and you obtain an average rating of 2.1 for a particular solution with mode="mean.class", this means that the (estimated) average rating for that solution was 2.1, or, a teensy bit above "useful". 

I'm using mode="mean.class" because I find it much easier to interpret an average rating (the sum of the probabilities of each of the three rating categories) than values on the arbitrary latent scale. 

N.B.: A warning to keep in mind when using mode="prob", and I assume it also applies to mode="mean.class": https://stats.stackexchange.com/questions/615711/why-are-emmip-response-y-axis-numbers-not-probabilities-for-ordinal-regressi#:~:text=There%20are%20several%20ways%20to,emmeans()%20or%20ref_grid()%20.&text=With%20mode%20=%20%22latent%22%20%2C,corresponding%20vector%20of%20coefficient%20estimates.
I think we will be okay as long as we include job in the estimate formula?

emmeans also gives you the option to weight the means by averaging over a factor. This handy command lets us see the weights in our model. https://stats.stackexchange.com/questions/610912/emmeans-weights-for-unbalanced-groups-factors
```{r}
ref_grid(fit1b)@grid
```

It appears that non-research staff are weighted more heavily, and students less so, presumably because there are a lot of observations for that group and not many for the other, respectively.
```{r}
sapply(
  c(
    "Students",
    "Non-research Staff",
    "Postdocs and Staff Researchers",
    "Faculty"
  ),
  function(x) {
    nrow(subset(combined, job_category == x))
  }
)
```

First, let's explore the outcomes with different weighting schemes. I'm not cherry picking here, I'm just trying to understand the options.
Let's calculate estimated marginal means for each solution, while holding job category constant. These will be really rough estimates, since we're averaging all the job categories, either equally or in proportion to their sample sizes. 

(Here's a somewhat helpful explanation of weights in emmeans: https://stackoverflow.com/questions/66748520/what-is-the-difference-between-weights-cell-and-weights-proportional-in-r-pa)

```{r}
# code copied from https://cran.r-project.org/web/packages/emmeans/vignettes/messy-data.html#weights
sapply(c("equal", "prop", "outer", "cells", "flat"), \(w)
    emmeans(fit1b, ~ solution, weights = w) |> predict()) |> head()
```

We only get two sets of estimates: equal/flat gives us the estimates where all means are given equal weight. Prop, outer, and cells give us another set of estimates, where each prediction is given the same weight as occurs in the model. At least, I think that's how it works. 

Let's look at the average ratings by job category (our weighting scheme here doesn't matter, because we're splitting it up by job, not averaging over job).

```{r}
summary(emmeans(fit1b, ~ solution | job_category, mode="mean.class")) %>%
 arrange(desc(mean.class))
```

Here, we see that "a learning community" is more popular among non-research staff than among other groups. So, we expect that if all groups are weighted equally, "a learning community" will be less popular than if we weight the means by sample size.

Hmm. I'm not sure why the following commands fail when we include mode="mean.class". It says no weighting information is given. Maybe it's just not possible to estimate the mean of the underlying probability distribution AND weight that mean at the same time?
```{r}
summary(emmeans(fit1b, ~ solution, weights = "equal")) %>% 
  arrange(desc(emmean))
```

```{r}
summary(emmeans(fit1b, ~ solution, weights = "prop")) %>% 
  arrange(desc(emmean))
```

Indeed, when we use the default weighting of "equal", "A learning community" is #8, but with "prop" weighting, it rises to #6. So now we know what the weights do.

In fact, the more that I think about it, the more I feel like we shouldn't even report the global emms--just emms by job. It may do more harm than good to average over a factor that we've already established is important. So let's look at emms by job.

```{r}
# This yields the same results: emmeans(fit1b, ~ solution | job_category, mode = "mean.class")
emm <- summary(emmeans(fit1b, ~ solution * job_category, mode = "mean.class"))
emm
```

# Plot emms
Plot the results.

```{r}
emm_clean <- emm %>% 
  rename(mean = mean.class,
         lwr  = asymp.LCL,
         upr  = asymp.UCL) %>% 
  mutate(across(c(mean, lwr, upr), as.numeric))

# Use a common ordering of solutions (here, overall mean w equal weighting)
solns_ordered <- summary(emmeans(fit1b, ~ solution, weights = "equal")) %>% 
  arrange(emmean) %>% # don't do desc() bc these will be flipped later w coord_flip()
  pull(solution) %>%
  as.character()

emm_clean <- emm_clean %>% 
  mutate(solution = factor(solution, levels = solns_ordered))
```

```{r, fig.width=10, fig.height=8}
make_plot <- function(df, jc) {
  ggplot(filter(df, job_category == jc),
         aes(x = solution, y = mean)) +
    geom_errorbar(aes(ymin = lwr, ymax = upr),
                  width = .15, linewidth = .4) +
    geom_point(size = 3) +
    ylim(c(1, 3)) +
    labs(title = jc, x = NULL, y = "Estimated mean rating (0â€“3)") +
    coord_flip() +                       # solutions run down the yâ€‘axis
    theme(
      plot.title = element_text(face = "bold"),
      axis.text.x = element_text(
        size = 12
      ),
      axis.text.y = element_text(
        size = 12
        ),
      panel.background = element_blank(),
      panel.grid = 
        element_line(
          linetype = "solid",
          color = "gray90"
          ),
      plot.margin = unit(c(0.3, 0.3, 0.3, 0.3), "cm")
    )
}

plots <- lapply(unique(emm_clean$job_category),
                make_plot, df = emm_clean)

composite_plot <- wrap_plots(plots, ncol = 2)

composite_plot
```

```{r}
save_plot("solns_points1.tiff", 12, 10, p=composite_plot)
```

Let's try combining them all on one plot.

```{r}
soln_levels <- levels(emm_clean$solution)
interleaved <- as.vector(rbind(paste0(soln_levels, "_sp"), soln_levels))
interleaved[length(interleaved)+1] <- "padding_sp"
```

```{r, fig.width=9, fig.height=6}
# Define a position dodge object to ensure points and error bars align
pd <- position_dodge(width = 0.6)

# one stripe per real category row
bg <- tibble(cat = factor(interleaved, levels = interleaved)) %>%
  mutate(
    ymin = as.numeric(cat) - 0.5,
    ymax = as.numeric(cat) + 0.5
  )
bg_even <- dplyr::filter(bg, row_number() %% 2 == 0)
bg_odd  <- dplyr::filter(bg, row_number() %% 2 == 1)


# Create the single, combined plot
p_final <- ggplot(emm_clean, 
       aes(x = solution, y = mean, 
           color = job_category, 
           shape = job_category,
           group = job_category)) +
# It's important that these rectangles are above the points and
# errors bars, so they'll the the bottom layer on the plot
  geom_rect(data = bg_odd,
          aes(xmin = ymin, xmax = ymax, ymin = -Inf, ymax = Inf),
          inherit.aes = FALSE, fill = "#f8f8f8", color = NA) +
  geom_rect(data = bg_even,
          aes(xmin = ymin, xmax = ymax, ymin = -Inf, ymax = Inf),
          inherit.aes = FALSE, fill = "#e6e6e6", color = NA) +
  geom_hline(yintercept = seq(1, 3, 0.5), color = "gray90") +
  geom_errorbar(aes(ymin = lwr, ymax = upr),
                width = 0.2, 
                linewidth = 0.5, 
                position = pd) +
  geom_point(size = 5, position = pd) +
  scale_shape_manual(values = c(16, 17, 15, 18)) +
  scale_x_discrete(limits = interleaved, breaks = soln_levels) +
  ylim(c(1, 3)) +
  labs(
    title = "Estimated Mean Rating by Job Category",
    x = NULL,
    y = "Estimated mean rating"
  ) +
  coord_flip() +
  theme(
    plot.title = element_text(size = 26, hjust = 0, face = "bold"),
    axis.text.x = element_text(size = 20),
    axis.text.y = element_text(size = 20),
    axis.title.x = element_text(size = 24),
    panel.background = element_blank(),
    #panel.grid.major.x = element_line(linetype = "solid", color = "gray90"),
    #panel.grid.major.y = element_line(linetype = "dashed", color = "gray95"),
    plot.margin = unit(c(0.3, 0.3, 0.3, 0.3), "cm"),
    legend.title = element_blank(),
    legend.text=element_text(size=20)
  )

p_final
```

```{r}
save_plot("solns_points2.tiff", 16, 10, p=p_final)
```


# Sanity checking: bar plots

I find it very surprising that "a learning community" ranked so low. Let's look at the rating distribution for each job category, for this solution, from the observed sample.

```{r, fig.width=10, fig.height=8}
learning_ratings <- grouped_bar_chart(
  df = subset(combined, solution=="A learning community"),
  x_var = "job_category",
  fill_var = "utility",
  title = "Ratings for 'A learning community'")
learning_ratings
```

```{r}
save_plot("solns_learning_comm.tiff", 12, 10, p=learning_ratings)
```

Ok, it's a messy plot but whatever. It shows that a lot of non-research staff selected "useful" or "very useful".

# Pairwise comparisons and p-values

```{r}
emm_job <- emmeans(fit1b, ~ job_category * solution, mode = "mean.class")
by_job <- summary(
  pairs(emm_job, by = "job_category"),
  infer = TRUE # infer CIs
)
head(by_job)
```

Here we look at pairwise contrasts by solution. These data are sort-of-kind-of corroborated below with a Kruskal-Wallis test.
```{r}
emm_soln <- emmeans(fit1b, ~ job_category * solution, mode = "mean.class")
by_soln <- summary(
  pairs(emm_soln, by = "solution"),
  infer = TRUE # infer CIs
) 

head(by_soln)
```


Let's glance at the significant differences.
```{r}
# Because there are so many significant comparisons,
# let's be stringent
sig_by_job <- subset(by_job, p.value < 0.0005)
sig_by_job
```

```{r}
sig_by_soln <- subset(by_soln, p.value < 0.05)
sig_by_soln
```

Okay, so here, the "estimate" column shows the difference in estimated marginal means for the two levels of interest, holding the other factor level constant (of my two factors, job and solution). So when the contrast is Computing environments vs. A learning community, the job_category is Faculty, and the estimate is 0.57, this indicates that the difference between the estimates of faculty's average rating of computer environments and their average rating of a learning community is 0.56, on a three-point scale.

```{r}
subset(
  summary(emm),
  job_category == "Faculty" & solution == "Computing environments"
)$mean.class -
  subset(
    summary(emm),
    job_category == "Faculty" & solution == "A learning community"
  )$mean.class
```

Sustainability grants and Finding funding show up frequently as being significantly higher than some of the other solutions. Let's plot the distributions of responses for sustainability grants, as a sanity check.

```{r, fig.width=10, fig.height=8}
grant_ratings <- grouped_bar_chart(
  df = subset(combined, solution=="Sustainability grants"),
  x_var = "job_category",
  fill_var = "utility",
  title = "Ratings for 'Sustainability grants'")
grant_ratings
```

```{r}
save_plot("solns_grants.tiff", 12, 10, p=grant_ratings)
```


# Kruskal-Wallis test for ranking differences between groups

Non-parametric corroboration of the extent of disagreement between groups. Whereas above, we tested for differences in mean ratings, here we are testing for differences in the distributions of ratings for each solution.

```{r}
combined2 <- combined %>%
  mutate(
    utility_score = recode(
      utility,
      "Non-applicable" = 0L,
      "Not very useful" = 0L,
      "Useful" = 1L,
      "Very useful" = 2L
    )
  )

kw_results <- sapply(split(combined2, combined2$solution), function(df) {
  kruskal.test(utility_score ~ job_category, data = df)$p.value
})

p_adj_kw <- p.adjust(kw_results, "holm")

p_adj_kw < 0.05
sum(p_adj_kw < 0.05)
```

Hm. The results are a little surprising. 


# Heat map (Unfinished/Abandoned)

I'd like to plot these data as a heat map. However, I find it really confusing to rely on the sign of the estimate (+/-) to tell me which solution is preferred. I'd rather have only positive values, and instead of using the sign to indicate which solution is preferred, we'll use the order of the solutions to indicate which solution is preferred. Let's say that the solution on the y-axis is always the preferred solution, so that the color/value merely indicates the extent to which respondents prefer the solution on the y-axis. 

To keep things simple for now, let's just look at non-research staff.

```{r}
nr <- subset(by_job, job_category == "Non-research Staff")
nr$contrast <- as.character(nr$contrast)
soln1 <- unname(sapply(nr$contrast, function(x) strsplit(x, " - ")[[1]][1]))
soln2 <- unname(sapply(nr$contrast, function(x) strsplit(x, " - ")[[1]][2]))
nr2 <- data.frame(
  soln1 = soln1,
  soln2 = soln2,
  value = nr$estimate,
  pval = nr$p.value,
  significant = ifelse(nr$p.value < 0.05, "*", "")
)
head(nr2)
```

```{r}
nr2_clean <- nr2 %>%
  # if value < 0 swap soln1/soln2 and flip the sign--one step at a time
  mutate(
    soln_pref = if_else(value < 0, soln2, soln1),   # preferred solution
    soln_other = if_else(value < 0, soln1, soln2),  # the other solution
    value_pos  = if_else(value < 0, -value, value)  # positive magnitude
  ) %>% 
  
  # keep only the modified columns
  select(
    soln1 = soln_pref,
    soln2 = soln_other,
    value = value_pos,
    significant
  )

#Check the before and after
tail(nr2)
tail(nr2_clean)
```

In this new data frame, soln1 is the preferred solution.

Let's reorder the factor levels and make them the same for both solution columns. This ensures that all solutions appear on both axes, in the same order.

```{r}
sol_levels <- sort(unique(c(nr2_clean$soln1, nr2_clean$soln2)))

plot_df <- nr2_clean %>% 
  mutate(
    soln1 = factor(soln1, levels = sol_levels),
    soln2 = factor(soln2, levels = sol_levels)
  ) %>% 
  # complete fills in missing combinations
  # as new rows with NA values
  complete(soln1, soln2)

plot_df
```

```{r, fig.width=12, fig.height=10}
nr_heatmap <- ggplot(
  data = plot_df,
  aes(x = soln2, y = soln1, fill = value)
) +
  geom_tile(width = 0.97, height = 0.97) +   # gaps for the grid to show
  geom_text(aes(label = significant), color = "black", size = 8) +
  # Reverse xâ€‘axis order and put it on top
  scale_x_discrete(limits = rev(sol_levels), position = "top") +
  scale_fill_gradient(low = "white", high = "darkred", na.value = "#e6e6e6") +
  ggtitle("Solutions preferred by non-research staff") +
  labs(
    fill = "Extent to which\ny-axis solution is\npreferred (difference in\nestimated mean rating)"
  ) +
  theme(
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    axis.text.x = element_text(
      angle = 46,
      vjust = 0,
      hjust = 0,
      size = 20
    ),
    axis.text.y = element_text(size = 20),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_blank(),
    legend.title = element_text(size = 15),
    panel.background = element_blank(),
    panel.grid = element_line(linetype = "solid", color = "gray90"),
    plot.title = element_text(
      hjust = 0.5,
      size = 24,
      margin = margin(b = 15)
    ),
    plot.margin = unit(c(0.3, 0.3, 0.3, 0.3), "cm")
  )

#nr_heatmap
```

Presumably, those warnings are just because our data frame contains a bunch of NA values, for combinations where the x-axis solution is preferred, and so we have no data.
```{r}
save_plot("solns_heatmap_nrstaff.tiff", 12, 10, p=nr_heatmap)
```

I'd like to "validate" this result by eyeballing a plot of the number of "Not very useful", "Useful", and "Very useful" results for non-research staff for a couple of solutions, say, sustainability grants and a learning community. The "favorite" solution question suggested that a learning community was more popular than grants among research staff, so the result here, which suggests grants are the most popular solution, is somewhat surprising.

```{r}
t <- long_data %>%
  filter(job_category == "Non-research Staff") %>%
  filter(solution == "Computing environments" |
        solution == "A learning community" |
        solution == "Sustainability grants" |
        solution == "Publicity")

```

```{r, fig.width=12, fig.height=10}
gbc <- grouped_bar_chart(
  df = t,
  x_var = "solution",
  fill_var = "utility",
  title = "Non-research staff solution preferences")

gbc
```

```{r}
save_plot("solns_sanity_check_nrstaff.tiff", 12, 10, p=gbc)
```

This looks reasonably consistent with the model. From this bar chart, only Sustainability grants and Computing environments have a strong, linear slope with "very useful" being the most common response; and of the two, that trend is more pronounced for Sustainability grants. Previously, I found that "A learning community" was the number 1 solution from non-research staff when we asked them to choose their top solution (see solutions_plots.qmd), but apparently when they were allowed to rate solutions on a Likert scale, Sustainability grants was more popular, basically.

This plot is pretty complicated and convoluted. If I do want to publish it, I'll probably want to use a diverging color scheme for positive and negative differences, and keep only squares on the diagonal gray. 


# Session Info
```{r}
sessionInfo()
```