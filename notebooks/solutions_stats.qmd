---
title: "Solutions Stats"
---

Some questions raised by my plots for Q10, about solutions (solutions_plots.qmd):
- Are solution scores by job category the same for all possible pairs of job groups?
- Are aspiring contributors significantly more likely than experienced contributors to select solutions related to learning and professional development?
- Are experienced contributors significantly more likely than aspiring contributors to select solutions related to funding?

Set seed
```{r}
set.seed(42)
```

# Import packages and utilities
```{r}
project_root <- here::here() # requires that you be somewhere in the
# project directory (not above it)
# packages
suppressMessages(source(file.path(project_root, "scripts/packages.R")))
# functions and objects used across scripts
suppressMessages(source(file.path(project_root, "scripts/utils.R")))
```

# Load data
```{r}
solutions <- load_qualtrics_data("clean_data/solutions_Q10.tsv")
other_quant <- load_qualtrics_data("clean_data/other_quant.tsv")
```

# Wrangle data
First, let's add a participant ID. To proceed with an ordinal regression model, we'll need to use a mixed model formula, since our observations are not independent: the individual participants are a random effect.
```{r}
solutions$participantID <- seq(1, nrow(solutions))
```

Next, remove empty rows, i.e. rows from respondents who didn't receive this question. As with many questions in this survey, we can cut some corners in the code because the question was mandatory. For example, no need to worry about incomplete answers.
```{r}
solutions_and_job <- solutions
solutions_and_job$job_category <- other_quant$job_category
names(solutions_and_job)[length(names(solutions_and_job))] <- "job_category"

nrow(solutions_and_job)
solutions_and_job <- exclude_empty_rows(solutions_and_job, strict=TRUE) # from scripts/utils.R
nrow(solutions_and_job)

head(solutions_and_job)
```


Convert to long data, since this makes it easier to remove NAs and is necessary for the statistics.
```{r}
long_data <- solutions_and_job %>%
  pivot_longer(
    cols = -c(participantID, job_category),
    names_to = "solution",
    values_to = "utility"
  )
dim(long_data)
head(long_data)
```

Remove NAs.
```{r}
long_data <- long_data %>%
    filter(!(utility == "Non-applicable"))
dim(long_data)
```

That removed about 200 rows, out of more than 2000. So less than 10% of the responses were "non-applicable"s.

Make utility an ordered factor. Solution and job category are not inherently ordered, but we'll make them factors, and the first factor level will be the reference level for that variable.
```{r}
long_data$utility <- factor(
  long_data$utility,
  levels = c("Not very useful", "Useful", "Very useful"),
  ordered = TRUE
)

long_data$solution <- factor(
  long_data$solution,
  levels = unique(long_data$solution)
)

long_data$job_category <- factor(
  long_data$job_category,
  levels = unique(long_data$job_category)
)

levels(long_data$solution)
levels(long_data$job_category)
```

Ok, so it looks like our reference levels are computing environments and faculty. That's fine. It doesn't really matter.

# Create candidate models
I'd like to fit a cumulative-logit mixed model, a.k.a. an ordinal regression model, using the clmm function from the ordinal package. (I am not using polr from the MASS package because it does not allow random effects.) I know we want to include participantID as a random effect, but I'm not 100% sure how to model solution. I think it would be best to compare different models.

Note that the next few cells take several minutes to run.

## Model 1: job_category * solution interaction
Here, I'm modeling job_category and solution as independent fixed effects, and assuming that there is also an effect from the interaction of the two. This way, we get a global slope (/coefficient/effect) for job_category, a global slope (/coefficient/effect) for solution, a global slope (/coefficient/effect) for the interaction (I think), and a global intercept. Adding participant as a random effect allows each participant to have their own deviation from the global intercept.
```{r}
fit1 <- clmm(utility ~ job_category * solution +      
              (1 | participantID),                 
            data = long_data, link = "logit", Hess = TRUE)
```
Hm. I get a warning that "Hessian is numerically singular: parameters are not uniquely determined" and "Absolute convergence criterion was met, but relative criterion was not met". The internet suggests that this might mean that some job-category × solution combinations have few or zero responses in one of the utility levels, so the full job_category * solution interaction is over-parameterised.

## Model 2: solution as a random effect, no correlation between participant intercept and job effect
Here's another formulation. In this case, solution is another random effect, so we only get one global slope from job_category, but each solution intercept (as well as each participant intercept) is allowed to deviate from the global intercept. We assume that across solutions, the deviations in job_cateogry effect from the global effect of job_category are not correlated with that solution's intercept's deviation from the global intercept.
```{r}
fit2 <- clmm(utility ~ job_category +       
              (1 | solution) +
              (1 | participantID) +
              (0 + job_category | solution),                
            data = long_data, link = "logit", Hess = TRUE)
```

Next, we again have 4 terms, like we did in the first model: a global intercept, slopes for job_category and solution, and a slope for the interaction. Now, we also estimate the deviance of each of these terms from the global baseline for each participant, and we also estimate the correlations between the deviations for each possible combination of the 4 terms, for each participant. Er, I think. (Helpful cheat sheet: https://stats.stackexchange.com/questions/13166/rs-lmer-cheat-sheet)

This one measures a ton of parameters... ABANDONED; NEVER CONVERGED
```{r}
# fit3 <- clmm(utility ~ job_category * solution +       # fixed effects
#               (0 + job_category*solution | participantID),
#             data = long_data, link = "logit", Hess = TRUE)
```

All the models seem to be struggling a bit. Let's explore the data for a moment.
```{r}
# three way cross tabs (xtabs) and flatten the table
# code from: https://ladal.edu.au/tutorials/regression/regression.html
ftable(xtabs(~ job_category + solution + utility, data = long_data))
```

Hm. Indeed, the data are sparse in places, particularly for undergraduates. Perhaps we should combine postdocs + staff researchers, as well as undergrads + grad students.
```{r}
combined <- long_data %>%
  mutate(
    job_category = recode(
      job_category,
      "Post-Doc" = "Postdocs and Staff Researchers",
      "Other research staff" = "Postdocs and Staff Researchers"
    )
  )

combined <- combined %>%
  mutate(
    job_category = recode(
      job_category,
      "Grad Student" = "Student",
      "Undergraduate" = "Student"
    )
  )
```

Now let's run models 1 and 2 again, but with this consolidated dataset.

## Model 1b: Model 1, but with consolidated data
```{r}
fit1b <- clmm(utility ~ job_category * solution +      
              (1 | participantID),                 
            data = combined, link = "logit", Hess = TRUE)
```

No warning this time, and I feel like it finished faster. My hunch is that this re-labeled dataset will lead to better results.

## Model 2b: Model 2, but with consolidated data
```{r}
fit2b <- clmm(utility ~ job_category +       
              (1 | solution) +
              (1 | participantID) +
              (0 + job_category | solution),                
            data = combined, link = "logit", Hess = TRUE)
```

So, those are two fairly complex models that I think capture the important variation. Let's compare them to some simpler models.

## Model 3: No job category
Let's make a null model where job category doesn't matter. (Using the consolidated data)
```{r}
fit3 <- clmm(utility ~ solution +      
              (1 | participantID),                 
            data = combined, link = "logit", Hess = TRUE)
```

## Model 4: No solution category
How about a model where solution doesn't matter?
```{r}
fit4 <- clmm(utility ~ job_category +      
              (1 | participantID),                 
            data = combined, link = "logit", Hess = TRUE)
```

## Model 5: job_category + solution
In this minimal model, we include job_category + solution, but without any interaction. I don't expect this model to be a good fit, because it would mean that all job categories gave each solution the same rating.
```{r}
fit5 <- clmm(utility ~ job_category + solution +
              (1 | participantID),                 
            data = combined, link = "logit", Hess = TRUE)
```

# Compare model AICs
```{r}
models <- list(
  "fit1"=fit1, # job_category * solution, sparser data
  "fit2"=fit2, # solution as random effect, sparser data
  "fit1b"=fit1b, # job_category * solution, denser data
  "fit2b"=fit2b, # solution as random effect, denser data
  "fit3"=fit3, # Null model: no job
  "fit4"=fit4, # Null model: no solution
  "fit5"=fit5 # Null model: no interaction
)
```

Let's check the AICs. You're not supposed to compare AICs for models fit to different data sets, but since I've only changed the job_category labels, but not the results themselves or the number of observations, I think this is ok.
```{r}
sapply(models, function(x) round(AIC(x)))
```

The AICs for all the models are fairly similar, except #4, where solution isn't doesn't matter, and job_category alone influences the response. This makes sense. Model 5, where job category and solution have no interaction, does fairly well. Maybe that interaction is subtle.

Model 1b looks the best. According to the interwebs, a delta AIC of more than ten is pretty substantial, and here we have a difference of 20 between the best and second-best.

Let's check the condition number of the Hessian. I don't really understand what this is, but the clmm2 tutorial says that high numbers, say larger than say 10^4 or 10^6, indicate poor fit.
```{r}
sapply(models, function(x) 
summary(x)$info["cond.H"]
)
```
By this metric, only fit1 is really bad.

The ordinal package provides goodness of fit functions nominal_test and scale_test, but these only work on clm objects, not clmm objects. Bummer.

Let's use an anova to compare the two models that scored the best. Since they also happen to be nested, an anova works here.
```{r}
anova(fit1b, fit5)
```

That's a very significant p-value. It looks like the interaction is worth including.

Model 2b had a similar AIC as model 5. While I can't compare model 1b and model 2b with anova, since they're not nested, I can at least glance at the standard errors of the coefficients, which give me a sense of the precision of the coefficient estimates.
```{r}
summary(fit1b$coefficients)
summary(fit2b$coefficients)
```

Hm. So fit1b had the lowest AIC of all the models and is significantly better at explaining the variation than the equivalent minimal model without an interaction term. However, fit2b has smaller SEs than fit1b, where solution is a random effect.

Ehh, I guess I'll go with fit1b? The AIC was much better, and I think this model also intuitively makes the most sense.

```{r}
summary(fit1b)
```

This is a lot to interpret. I'll do my best. First, let's just at the main effects, i.e. the effects of job category and solution. In the summary above, each job category is compared to Faculty, our job reference level, for the solution Computing environments, our solution reference level. The "Estimate" for job_categoryStudent is 1.66906, which indicates students have odds of e^1.67=5.3 of rating that solution at least one category higher than faculty. 

The solution Publicity has a coefficient of -0.66811, indicating that faculty have odds of e^0.67=2 of rating Publicity one level lower than Computing Environments.

The interactions, e.g. job_categoryPostdocs and Staff Researchers:solutionPublicity, indicate extra log-odds only for that specific job × solution pair beyond the two main effects. So in that example, postdocs and staff researchers have an extra log-odds of 0.78228 (odds of e^0.78228=2.186) of giving publicity a higher rating than computing environments, as compared to faculty.

Interestingly, none of our p-values are super significant for interactions, meaning none of the interactions are really significant on their own. These were the most significant effects (three asterisks):
Coefficients:
                                                                         Estimate Std. Error z value Pr(>|z|)
solutionA learning community                                             -1.56910    0.38854  -4.038 5.38e-05 ***
solutionEvent planning                                                   -1.83275    0.40245  -4.554 5.26e-06 ***
solutionMentoring programs                                               -1.93070    0.39813  -4.849 1.24e-06 ***
solutionEducation                                                        -1.68608    0.39749  -4.242 2.22e-05 ***
solutionSustainability grants                                             1.73451    0.44904   3.863 0.000112 ***

So, faculty had significantly higher odds for sustainability grants than computing environments; significantly lower for education than computing environments, etc.

One job category did get two asterisks:
Coefficients:
                                                                         Estimate Std. Error z value Pr(>|z|) 
job_categoryStudent                                                       1.66906    0.61824   2.700 0.006941 ** 

So, students had somewhat significantly higher odds of selecting computing environments than faculty.


```{r}
# In the future, we may want to get results on a different scale from the log odds scale
# By default, "mode" is "latent", but we can change this to e.g. mode="prob",
# which will give the probabilities of each class of ordinal response.
# I haven't implemented this because I don't fully understand it--need to read more.
emm <- emmeans(fit1b, ~ job_category * solution)
by_job <- summary(
  pairs(emm, by = "job_category"),
  infer = TRUE # infer CIs
) 
# sig_by_job <- subset(by_job, p.value < 0.05)
# sig_by_job
```


Okay, so here, the "estimate" column shows the difference in estimated marginal means for the two levels of interest, holding the other factor level constant (of my two factors, job and solution). So when the contrast is Computing environments vs. A learning community, the job_category is Faculty, and the estimate is 1.57, this indicates that faculty have a e^1.57 = 4.8 higher odds of rating Computing environments at least one category higher than A learning community. In other words, the odds of a faculty rating Computing environments at least one category higher than A learning community are 4.8:1.

I'd like to plot these data as a heat map. I find it really confusing to rely on the sign of the estimate (+/-) to tell me which solution is preferred. I'd rather have only positive values, and instead of using the sign to indicate which solution is preferred, we'll say that the solution on the x-axis is always the preferred solution, so that the color/value merely indicates the extent to which respondents prefer the solution on the x-axis. 

TODO: SWITCH THIS TO Y-AXIS FOR VISUAL CLARITY

```{r}
nr <- subset(by_job, job_category == "Non-research Staff")
nr$contrast <- as.character(nr$contrast)
soln1 <- unname(sapply(nr$contrast, function(x) strsplit(x, " - ")[[1]][1]))
soln2 <- unname(sapply(nr$contrast, function(x) strsplit(x, " - ")[[1]][2]))
nr2 <- data.frame(
  soln1 = soln1,
  soln2 = soln2,
  value = nr$estimate,
  significant = ifelse(nr$p.value < 0.05, "*", "")
)
head(nr2)
```

```{r}
nr2_clean <- nr2 %>%
  # if value < 0 swap soln1/soln2 and flip the sign--one step at a time
  mutate(
    soln_pref = if_else(value < 0, soln2, soln1),   # preferred solution
    soln_other = if_else(value < 0, soln1, soln2),  # the other solution
    value_pos  = if_else(value < 0, -value, value)  # positive magnitude
  ) %>% 
  
  # keep only the modified columns
  select(
    soln1 = soln_pref,
    soln2 = soln_other,
    value = value_pos,
    significant
  )
tail(nr2)
tail(nr2_clean)

# exponentiate log odds to odds
nr2_clean$value <- exp(nr2_clean$value) 
```

```{r}
ggplot(data = nr2_clean, aes(x = soln1, y = soln2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "darkred") +
  ggtitle("Solutions preferred by non-research staff") + 
  labs(fill="Extent to which\nsolution on x-axis\nis preferred\n(odds ratio)") +
  theme(
      axis.title.x = element_blank(),
      axis.title.y = element_blank(),
      axis.text.x = element_text(
        angle = 90,
        vjust = 0.6,
        size = 16
      ),
      axis.text.y = element_text(size = 16),
      axis.ticks.x = element_blank(),
      axis.ticks.y = element_blank(),
      legend.title = element_text(size=14),
      panel.background = element_blank(),
      panel.grid = element_line(linetype = "solid", color = "gray90"),
      plot.title = element_text(
        hjust = 0.5,
        size = 24,
        margin = margin(b = 15)
      ),
      plot.margin = unit(c(0.3, 0.3, 0.3, 0.3), "cm")
  )
```

TODO: "Validate" this result by eyeballing: plot the number of "Not very useful", "Useful", and "Very useful" results for non-research staff for a couple of solutions, say, sustainability grants and a learning community. The "favorite" solution question suggested that a learning community was more popular than grants among research staff, so the result here, which suggests grants are the most popular solution, is somewhat surprising.