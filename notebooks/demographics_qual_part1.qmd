---
title: "Demographics: qualitative responses"
---

TODO:
- Remember we asked people to comma-separate multiple fields
- Need a smarter algorithm? Or just go through them manually. Probably the latter


# Qualitative responses: subfields

For Q18, after asking respondents their broad domain of study, we asked them this free-response question:\
What is your primary field? (e.g. astrophysics, neuroscience)\
One response is preferred, but if multiple are needed, please separate with commas.\

I plan to "classify" these free responses into standardized categories. The taxonomy of academic disciplines that I'll be using is here:\
https://digitalcommons.elsevier.com/en_US/dc-disciplines-taxonomy

I downloaded the PDF and did a very tiny amount of curation to make this list of academic fields and subfields machine-readable.

I'll use a fuzzy string matching algorithm to classify the responses--essentially, looking for which item in the taxonomy most closely matches the user's entry. Hopefully, there will be very few entries that aren't a good match for anything in the taxonomy, and I can deal with these stragglers manually.

# Import packages and utilities
```{r}
project_root <- here::here() # requires that you be somewhere in the
# project directory (not above it)
# packages
suppressMessages(source(file.path(project_root, "scripts/packages.R")))
# functions and objects used across scripts
suppressMessages(source(file.path(project_root, "scripts/utils.R")))
```

# Define funtions

```{r}
get_all_hits <- function(query, tax_df) {
    n_tax <- nrow(tax_df)

  # Pre-allocate data frame rows for computational efficiency
  query_scores <- data.frame(
    min_dist = rep(NA_real_, n_tax),
    level = rep(NA_character_, n_tax),
    index = seq_len(n_tax),
    stringsAsFactors = FALSE
  )

  # Get metadata for all hits with the minimum distance score
  for (i in seq_len(n_tax)) {
    results <- get_row_results(query, tax_df, i)
    query_scores$min_dist[i] <- as.numeric(results[[1]])
    query_scores$level[i] <- results[[2]]
  }

  return(query_scores)
}
```


```{r}
get_row_results <- function(query, tax_df, row_num) {

  # For a given row/query pair (the query being the
  # string our respondent wrote on the survey),
  # Record which column had the lowest distance score,
  # and the string in that column.
  # I'm not currently handling ties because... I'll
  # deal with that if and when it arises.

  # I like to include tax_df as an argument just for clarity,
  # even though tax_split is the only df I'll be running this on.

  strL1 <- tax_df[row_num, "Level1"]
  strL2 <- tax_df[row_num, "Level2"]
  strL3 <- tax_df[row_num, "Level3"]

  # compute Levenshtein distances for each level
  dist1 <- as.numeric(
    adist(query, strL1)
  )
  dist2 <- as.numeric(
    ifelse(
      !is.na(strL2),
      adist(query, strL2),
      Inf
    )
  )
  dist3 <- as.numeric(
    ifelse(
      !is.na(strL3),
      adist(query, strL3),
      Inf
    )
  )

  # assemble into a vector
  min_dist = min(c(dist1, dist2, dist3), na.rm = TRUE)
  level = c("Level1", "Level2", "Level3")[
    which.min(c(dist1, dist2, dist3))
  ]

  return(
    c(min_dist, level)
  )
}
```

```{r}
get_candidates <- function(q_scores) {
  # For a given row/query pair (the query being the
  # string our respondent wrote on the survey),
  # collect indices for all rows that had the lowest
  # distance score, as well as column indices (though
  # I am using column names e.g. "Level1" instead of numbers).

  # find the global minimum across all rows
  best_dist <- min(q_scores$min_dist)
  cands <- q_scores %>% filter(min_dist == best_dist)
  return(cands)
}
```


```{r}
get_winner_df <- function(cands_df) {
  winners <- ""
  lvl3_cands <- subset(cands_df, level == "Level3")
  lvl2_cands <- subset(cands_df, level == "Level2")
  lvl1_cands <- subset(cands_df, level == "Level1")

  if (nrow(lvl3_cands) > 0) {
    winners <- tax_split[lvl3_cands$index, ]
  } else if (nrow(lvl2_cands) > 0) {
    keep_idx <- lvl2_cands$index[is.na(tax_split$Level3[lvl2_cands$index])]
    winners <- tax_split[keep_idx, , drop = FALSE]
  } else {
    keep_idx <- lvl1_cands$index[
      is.na(tax_split$Level2[lvl1_cands$index]) &
        is.na(tax_split$Level3[lvl1_cands$index])
    ]
    winners <- tax_split[keep_idx, , drop = FALSE]
  }
  return(winners)
}
```

# Load data

```{r}
other_quant <- load_qualtrics_data("clean_data/other_quant.tsv")
status <- load_qualtrics_data("clean_data/contributor_status_Q3.tsv")
qual <- load_qualtrics_data("qual_responses.tsv")
tax <- as.data.frame(
    readLines("data/digital_commons_disciplines.txt"),
    stringsAsFactors = FALSE)

data <- cbind(status, other_quant)
nrow(data)
head(data)
```

```{r}
tmp <- data$job_category[nzchar(data$job_category)]
job_count <- data.frame(table(tmp))
names(job_count) <- c("Job", "Count")

academics <- sum(subset(job_count, Job != "Non-research Staff")[, "Count"])
```

```{r}
qual_fields <- qual$subfield[nzchar(qual$subfield)]
length(qual_fields)
academics
```

This question was not mandatory, but 170/188 academics answered it.

```{r}
head(qual_fields)
```

Let's standardize the capitalization in this vector. The capitalization will now match my taxonomy, which is important since the fuzzy string matching algorithm I'll be using is case-sensitive.

```{r}
qual_fields <- unname(
    sapply(
        qual_fields,
        function(x) tools::toTitleCase(tolower(x)) )
    )

head(qual_fields)
```

Oof. The correction of "AI" to "Ai" is unfortunate, but let's see how it goes.

Here's our taxonomy:

```{r}
head(tax)
```

Let's tidy this data frame.

```{r}
tax <- tax %>%
  separate(
    col   = names(tax)[1],
    into  = c("Level1", "Level2", "Level3"),
    sep   = ": ",
    fill  = "right",   # any missing pieces become NA
    extra = "merge"    # if there were >2 colons, theyâ€™d all merge into Level3
  )

head(tax)
```

You can take my word for it that I looked back and forth between the data frame and the data file, and made sure that the data frame looks good. (Correct # of rows, row numbers match line numbers, etc.)

Cool! Now we are ready for fuzzy string matching. Let's start with a very rudimentary method, and we can do something fancier if this appears insufficient.

We are just using the adist() function in base R to calculate the Levenshtein distance between pairs of strings--the minimum number of substitutions needed to turn 'string a' into 'string b'.

Since we invited people to separate multiple disciplines with a comma, we'll need to parse those and track participant IDs. Several people used a slash instead of a comma, so we'll use a regex that looks for that, too.
```{r}
# I think the data need to be a tibble for unnest to work

responses_df <- tibble(
  participantID = seq_along(qual_fields),
  response = strsplit(qual_fields, "\\s*[,/]\\s*", perl = TRUE)
  # \\s* matches zero or more whitespace (first backslash is an escape)
  # [,/] will match either a comma or a slash
  # With base R strsplit(), \\s only works if you set perl = TRUE
) %>%
  unnest_longer(response) %>% # takes a column of type list, where the entries
  # are themselves vectors, and turns each element of those vectors
  # into its own row, duplicating the other fields appropriately.
  mutate(response = trimws(response)) %>%     # strip whitespace around each field
  filter(response != "")                   # drop empty entries (from e.g. A,,B)

responses_df
```

Again, you can take my word for it that I visually compared this df to the raw survey data and it looks good.

```{r}
results <- vector("list", nrow(responses_df))

for (i in seq_len(nrow(responses_df))) {
  current_query <- responses_df$response[i]
  current_participantID <- responses_df$participantID[i]

  q_scores <- get_all_hits(current_query, tax)
  candidates <- get_candidates(q_scores)
  winnerdf <- get_winner_df(candidates)

  # add participant info
  winnerdf$participantID <- current_participantID
  winnerdf$response <- current_query
  results[[i]] <- winnerdf
}

# combine results into a data frame
final_results <- dplyr::bind_rows(results) %>%
  # move participantID to be the first column
  dplyr::relocate(participantID, response)

head(final_results, n=50)
```

Okay, well I got results, but there's a pretty high rate of crappy matches. And then there's some that only a sophisticated algorithm would catch, and a couple that I'M not even sure how to classify. I think experimenting with better algorithms, or "real ML" classifiers, would probably take longer than just manually sifting through these. My code is ugly, and I hate that my procedure is not reproducible, but it's fast. So I'm just going to manually review these classifications in Microsoft Excel.

```{r}
write.table(final_results, "data/qual_fields_guesses.tsv", sep="\t", quote=FALSE, row.names = FALSE)
```

