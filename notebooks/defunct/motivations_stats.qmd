---
title: "Motivations for contributing to OS: statistical analysis"
---

# Overview
This script runs some statistical tests on data from Q6, which is about participants' reasons for contributing to open source.

THIS SCRIPT IS AN OLD VERSION OF AN ANALYSIS I LATER RE-DID. Accordingly, I am commenting out references to mvabund, so that renv::snapshot will not include this package in the lock file. IF YOU WANT TO RUN THIS SCRIPT, YOU'LL NEED TO UNCOMMENT THOSE.

# Import packages and utilities
```{r}
project_root <- here::here() # requires that you be somewhere in the
# project directory (not above it)
# packages
suppressMessages(source(file.path(project_root, "scripts/packages.R")))
# functions and objects used across scripts
suppressMessages(source(file.path(project_root, "scripts/utils.R")))
```

# Set seed
```{r}
set.seed(42)
```

# Function definition
#### pairwise_z_test_lessthan
- Arguments:
  - `df`: A data frame where rows are participants, and columns are a
    predictor, either `job_category` or `Role`, plus at least one response 
    variable, e.g. `Skills`, `Give back`, etc. Extra columns are okay. 
    The response variable of interest should be a column of 0s and 1s.
  - `outcome_col`: A string. `Skills` by default, but could be any of the
    7 response variables, e.g. `Improve tools`, `Job`, etc.
  - `predictor_col`: A string. The name of a factor column containing 2 groups
    to compare. Currently, should be either `Role` or `job_category` (default).
  - `group1`: A string. The job category that you suspect has a lower
    "success rate", of the two.
  - `group2`: A string. The job category that you suspect has a higher
    "success rate", of the two.
- Details:
  - A simple function that performs a pairwise z-test for equality of two
    proportions. Most of the function is just summing across the data frame.
    it calls stats::prop.test to run a one-sided z-test testing whether
    the proportion of "successes" in group1 is less than that of group2.
- Outputs:
  - An "htest" object, from stats::prop.test.
```{r}
pairwise_z_test_lessthan <- function(
  df,
  outcome_col = "Skills",
  predictor_col = "job_category",
  group1,
  group2,
  alternative = "less"
) {
  # Count total and 'yes' outcomes for each group
  n1 <- sum(df[[predictor_col]] == group1)
  y1 <- sum(df[[predictor_col]] == group1 & df[[outcome_col]] == 1)

  n2 <- sum(df[[predictor_col]] == group2)
  y2 <- sum(df[[predictor_col]] == group2 & df[[outcome_col]] == 1)

  # Perform the one-sided prop test (testing if group1 < group2)
  result <- prop.test(
    x = c(y1, y2),
    n = c(n1, n2),
    alternative = alternative,
  )

  return(result)
}
```


# Load data
```{r}
motivations <- load_qualtrics_data("clean_data/motivations_Q6.tsv")
other_quant <- load_qualtrics_data("clean_data/other_quant.tsv")
```

# Wrangle data

```{r}
motivations_job_staff <- cbind(motivations, other_quant$job_category)
# Rename columns
names(motivations_job_staff)[length(names(motivations_job_staff))] <- "job_category"
motivations_job_staff <- cbind(motivations_job_staff, other_quant$staff_categories)
names(motivations_job_staff)[length(names(motivations_job_staff))] <- "staff_category"
# Remove any rows where the job_category or staff_category are missing
motivations_job_staff_clean <- exclude_empty_rows(motivations_job_staff, strict=TRUE)
# Remove rows of all 0s
motivations_job_staff_clean <- motivations_job_staff_clean %>%
  filter(!if_all(Job:Other, ~ .x == 0))
head(motivations_job_staff_clean)

# Do the same, but dropping staff categories (e.g. IT)
motivations_job <- subset(motivations_job_staff, select=-staff_category)
# Remove any rows where the job_category is missing
motivations_job_clean <- exclude_empty_rows(motivations_job, strict=TRUE)
# Remove rows of all 0s
motivations_job_clean <- motivations_job_clean %>%
  filter(!if_all(Job:Other, ~ .x == 0))
head(motivations_job_clean)

# This will also come in handy later.
motivation_cols <- names(motivations_job_clean)[-length(names(motivations_job_clean))]
motivation_cols
```

Here, we use some functions in my utility script (scripts/utilities.R) to clean up the data for Q6. We'll call the resulting data frame `motivations_raw`. This data frame also has a `Role` column indicating the participant's job category.

Here, we combine postdocs and other research staff into one category. We'll call the resulting data frame `motivations_processed`. We will use this for most of our statistical analysis. It gives us more statistical power, and I think it is reasonable in terms of interpretability.

```{r}
motivations_job_clean_streamlined <- motivations_job_clean %>%
  mutate(
    job_category = case_when(
      job_category %in% c("Post-Doc", "Other research staff") ~ 
        "Postdocs and Staff Researchers",
      TRUE ~ job_category
    )
  )
```

# Create the regression model
I'm interested in the whether a person's job category affects how they will answer this question. In other words, can we predict their profile of motivations significantly better when taking job category into account? I am doing a multivariate logistic regression predicting a vector of binary responses. It's multivariate because instead of doing Y ~ X, we are now doing [Y1, Y2, Y3...] ~ X. It's logistic because all response variables are binary. I'm using the mvabund() package, which is designed for non-continuous data: counts and binary outcomes (they call these "abundance data" in the packaage documentation). Base R's lm() is for continuous data, and I didn't see a function in base R for this type of analysis.

First, we just split our data frame into two. For each observation, X is a single categorical outcome, and we have seven Ys which are binary outcomes.
```{r}
Y <- as.matrix(motivations_job_clean_streamlined[, motivation_cols])
X <- motivations_job_clean_streamlined$job_category
head(Y)
head(X)
```

Create the model. I'm mostly using the default settings.
```{r}
#fit <- mvabund::manyglm(Y ~ X, family = "binomial", show.coef=TRUE)
fit
```

Immediately, we notice that the coefficients for Undergraduates on "Skills" and "Give back" are very different from all other coefficients. This is presumably because all 7 undergraduates who answered this question selected both those options.

The residual deviance statistic is close to the degrees of freedom, which I think suggests a good fit? https://online.stat.psu.edu/stat504/lesson/2/2.5

This is a really dumb way to assess goodness of fit, but I also like to look at the AIC, and if it's in the thousands, I start to get nervous. Ours are in the hundreds, so that seems promising?

# Assess goodness of fit
Next, I'm eyeballing some plots to assess goodness of fit.

The distribution of the residuals seems normal-ish (shape varies with my chosen random seed). Note I'm just picking one of the categories that had a weird coefficient, to make sure the data in that category aren't too weird.
```{r}
pit_resids <- residuals(fit, type = "pit.trap")
hist(pit_resids[, "Skills"], main = "PIT Residuals: Skills")
```

Admittedly, I don't fully understand this next plot, but the mvabund paper emphasizes it. (https://doi.org/10.1111/j.2041-210X.2012.00190.x) I think the point is that the residuals should be distributed around zero. What you don't want is a "fan shape", where as the predictions get more extreme, the residuals do, too. This tutorial from the package author shows an example of this. https://cran.r-project.org/web/packages/ecostats/vignettes/Chapter14Solutions.html I think my plot look as good as the plots he approves of in that tutorial.
```{r}
plot(fit)
```

I think a correlation matrix is also applicable here. The correlations between variables are near zero (ish), suggesting that the model captures the important relationships.
```{r}
corrplot(cor(pit_resids), method = "circle")
```

# Hypothesis testing

Now we want to know whether this model is significantly better than a model where the probability of a particular set of motivations is the same for all job categories. anova() summarizes the statistical significance of the fitted model. test="LR" is the default, and specifies a likelihood ratio test. So I guess we are using the likelihood ratio test statistic instead of the standard anova F-statistic, but I think this might be the kind of situation where those two statistics are basically the same? (nested models for hypothesis vs. null) The resamp="pit.trap" ("probability integral transform" residuals) argument is the default resampling method. I think the function resamples the data to get a null distribution.

## Global model fit + univariate tests
```{r}
anova_result <- anova(fit, resamp = "pit.trap", test = "LR", p.uni = "adjusted")
anova_result
```

The anova output starts with a table of the multivariate test statistics. This tests for the global effect of Role, by resampling the whole response vector.

The next part of the table is the univariate test statistics, which are separate logistic regressions for each response variable, ignoring the other variables.

Our Pr(>Dev) is a statistically significant p-value, indicating that Role significantly predicts motivation profile. Basically, the model including role better explains the data than the null model.

Only "Skills" shows a significant univariate effect of Role after multiple testing correction (p.uni = "adjusted"). In other words, when considering whether Role can predict a single motivation, it can only predict Skills.

I think this also means that we have enough umdergraduates, because if we didn't, we wouldn't have enough statistical power to reject the null hypothesis, right?

## Pairwise tests for job categories

I believe we can use the pairwise.comp argument to test whether pairs of categories in our explanatory variable are significantly different from each other.

```{r}
anova_pw <- anova(
  fit,
  resamp = "pit.trap",
  test = "LR",
  p.uni = "adjusted",
  pairwise.comp = X
)
anova_pw
```

The results indicate three significant pairwise comparisons:\
* Faculty vs Undergraduate\
* Postdocs and Staff Researchers vs Undergraduate\
* Faculty vs Non-research Staff\

# Test for trend in "skills"

In my other script, motivations_plots, we have one plot where we apparently see a trend: the probability of a respondent choosing "skills" as a motivator appears to decrease as they advance in their academic career. We will use a Cochrane-Armitage test for trend to evaluate whether this trend is real. More precisely, I believe we are evaluating whether the order "P(Yes | Undergrad) > P(Yes | Grad) > P(Yes | Postdoc) > P(Yes | Faculty)" is highly unlikely (<95% chance) given the null hypothesis that all four categories have the same probability of a "yes" response.

Full disclosure: I'm being a little p-hacky here, because I'm only trying this after I tried a series of pairwise z-tests to see whether the proportion of "yes" for "skills" was significantly different from undergrads vs. grads, grads vs. postdocs, etc. That analysis is after this section. In all seriousness, I don't actually feel that I am p-hacking because I'm not just using a new test to try and make the same claim; this is a different test and we will interpret it appropriately. I'm not claiming that undergrads are more likely than grads to select skills; I'm just claiming that there is a trend across the 4 categories. 

```{r}
# Here, I haven't combined post-docs and other research staff
n_postdoc <- sum(motivations_job_clean$job_category == "Post-Doc")
n_postdoc_yes <- sum(
  motivations_job_clean$job_category == "Post-Doc" &
    motivations_job_clean$Skills == 1
)
# For the other groups, it doesn't matter if we use the raw or processed data
n_faculty <- sum(motivations_job_clean$job_category == "Faculty")
n_faculty_yes <- sum(
  motivations_job_clean$job_category == "Faculty" &
    motivations_job_clean$Skills == 1
)

n_grad <- sum(motivations_job_clean$job_category == "Grad Student")
n_grad_yes <- sum(
  motivations_job_clean$job_category == "Grad Student" &
    motivations_job_clean$Skills == 1
)

n_undergrad <- sum(motivations_job_clean$job_category == "Undergraduate")
n_undergrad_yes <- sum(
  motivations_job_clean$job_category == "Undergraduate" &
    motivations_job_clean$Skills == 1
)

n_yes <- c(
  n_undergrad_yes,
  n_grad_yes,
  n_postdoc_yes,
  n_faculty_yes
)

n_tot <- c(
  n_undergrad,
  n_grad,
  n_postdoc,
  n_faculty
)

# Assign scores 1,2,3,4 for Undergrad --> Faculty
# To indicate the ordering
scores <- 1:4

stats::prop.trend.test(
  x = n_yes,
  n = n_tot,
  score = scores
)
```

I'm honestly not sure whether this is a one-tailed or two-tailed test... I would assume one-tailed, but the documentation is terse. Anyway, even if we divide that p-value by two it's still well under p=0.05.

# Negative/abandoned analysis: Pairwise z-tests

I also looked at that skills trend from a different perspective: is each pair of consecutive categories significantly different? So, are faculty significantly less likely to select "Skills" than postdocs, are postdocs significantly less likely than grad students to select it, etc.?

The results of this analysis are both less significant and harder to interpret than the trend test, but I'm including it for posterity. 

I also did some post-hoc power analyses, because we have small sample sizes: 15 postdocs and 7 undergraduates. If we fail to reject the null hypothesis, it could just be because we lack statistical power. Here are some links that I based this on:\
https://rpubs.com/sypark0215/223385 \
https://cran.r-project.org/web/packages/pwr/vignettes/pwr-vignette.html \
Essentially I am asking: what ratio of group1:group2 is needed to achieve 80% power?

### Graduate students vs. Undergraduates

Let's start with the power analysis. 

First, let's prepare the proportions we'll need to run the power test. We might as well do this for all four job categories of interest.
```{r}
# If this were python I would make a class, but I'm not that good
# at R coding so I'm just making a bunch of variables LOL.

p_grad_yes <- n_grad_yes / n_grad
p_undergrad_yes <- n_undergrad_yes / n_undergrad
p_faculty_yes <- n_faculty_yes / n_faculty
p_postdoc_yes <- n_postdoc_yes / n_postdoc
```

Calculate Cohen's h, the effect size.
```{r}
h <- pwr::ES.h(p_grad_yes, p_undergrad_yes)
```

Now, what ratio of n_undergrads to n_gradstudents is needed to achieve 80% power? This one-sided test allows us to specify our unequal group sizes.
```{r}
pwr::pwr.2p2n.test(
  h = h,
  n1 = n_grad,
  sig.level = 0.05,
  power = 0.8,
  alternative = "less"
)
```

So we would need 10.5 undergrads to achieve 80% power. Alas, we only have 7. So there is no point in proceeding with the hypothesis test.

### Postdocs vs. Graduate students

Calculate Cohen's h, the effect size.
```{r}
h <- pwr::ES.h(p_postdoc_yes, p_grad_yes)
```

```{r}
tryCatch(
  pwr::pwr.2p2n.test(
    h = h,
    n1 = n_grad,
    sig.level = 0.05,
    power = 0.8,
    alternative = "greater"
  ),
  error = function(e) e
)
```

This test fails to give an answer. I think the problem is that the difference in proportions is so small (81% vs. 73%), and the number of grad students is also so small (26), that we will never have enough postdocs to achieve 80% power. With a Cohen's h of 0.5 or greater, it says we would need at least 500 postdocs. With h less than 0.5, the function breaks. So if the absolute value of the effect size were larger, we would have more power, which makes sense. I could plot this function for various h values, but honestly I don't care.
```{r}
pwr::pwr.2p2n.test(
  h = 0.5,
  n1 = n_grad,
  sig.level = 0.05,
  power = 0.8,
  alternative = "greater"
)
```


### Faculty vs. Postdocs

Calculate Cohen's h, the effect size.
```{r}
h <- pwr::ES.h(p_faculty_yes, p_postdoc_yes)
```

```{r}
pwr::pwr.2p2n.test(
  h = h,
  n1 = n_faculty,
  sig.level = 0.05,
  power = 0.8,
  alternative = "less"
)
```
We have 15 postdocs, and we need 15.6 postdocs for a one-sided test. Good enough.

```{r}
pairwise_z_test_lessthan(
  motivations_job_clean, # Note we are just looking at post docs
  group1 = "Faculty",
  group2 = "Post-Doc"
)
```

It appears that faculty are significantly less likely than postdocs to select "Skills" as a motivator.

# By popular demand: IT vs. Academics

Greg raised an interesting question: what about IT staff vs. academics? Let's play around with this. It's basically the same question we had before, but now our predictor variable only has two categories.

I plotted the data (see motivations_plots.qmd), and it appears that these groups are somewhat different. The "Job" motivation looks to be the most different, just by eyeballing it. But let's see what the statistics say.

## Data Wrangling
```{r}
it <- motivations_job_staff_clean %>%
  filter(staff_category == "Information Technology (IT)") %>%
  select(-c(job_category, staff_category))
it$Role <- "IT"
head(it)
dim(it)
```

```{r}
# Everyone except non-research staff
academics <- motivations_job_clean_streamlined %>%
  filter(
    job_category == "Faculty" |
    job_category == "Grad Students" |
    job_category == "Postdocs and Staff Researchers" |
    job_category == "Undergraduates"
  ) %>%
    select(-job_category)
academics$Role <- "Academic"
head(academics)
dim(academics)
```

```{r}
it_acad <- rbind(it, academics)
it_acad$Role <- as.factor(it_acad$Role)
dim(it_acad)
```

## Regression
Let's try the logistic regression. This model may be more powerful than we need, but since I already have the code, let's just do it.
```{r}
Y <- as.matrix(it_acad[, motivation_cols])
X <- it_acad$Role
```

I'm just glancing at the AIC values, degrees of freedom, and the residual plot. They seem fine. It would be a bit weird if they weren't, since this is the same data as before; the categories are just grouped differently (and in fact, our minimum sample size is larger).
```{r}
#fit <- mvabund::manyglm(Y ~ X, family = "binomial", show.coef=TRUE)
fit
plot(fit)
```

```{r}
anova_result <- anova(fit, resamp = "pit.trap", test = "LR", p.uni = "adjusted")
anova_result
```

ANOVA/LR test shows the groups are significant. They help explain the data.

We can also look at the Pr(>Dev) for each univariate test, which I'm assuming are p-values. These show that the only motivation that is predicted on its own by these two groups is "Job", with a p-value of 0.02.

```{r}
tryCatch(
  anova_pw <- anova(
    fit,
    resamp = "pit.trap",
    test = "LR",
    p.uni = "adjusted",
    pairwise.comp = X
  ),
  error = function(e) e
)
```

Uh-oh. The pairwise comparison fails. Well, actually, that makes sense. The two groups must be different from each other, or else the global model fit wouldn't be significant. So we actually don't need this test.

## Power analysis
So, we already know that the IT/Academic distinction is a pretty good predictor of the "Job" motivation. But since I already have the code for the z-test of proportions, I kind of want to do that test, too, just out of curiosity. I feel pretty confident that the regression is working well, but I will feel even better if the z-test is significant, too.

```{r}
n_it <- sum(it_acad$Role == "IT")
n_it_yes <- sum(
  it_acad$Role == "IT" &
    it_acad$Job == 1
)
p_it_yes <- n_it_yes / n_it

n_acad <- sum(it_acad$Role == "Academic")
n_acad_yes <- sum(
  it_acad$Role == "Academic" &
    it_acad$Job == 1
)
p_acad_yes <- n_acad_yes / n_acad

paste0("IT total: ", n_it)
paste0("IT yes to 'Job': ", n_it_yes)
paste0("IT proportion yes: ", p_it_yes)

paste0("Academics total: ", n_acad)
paste0("Academics yes to 'Job': ", n_acad_yes)
paste0("Academics proportion yes: ", p_acad_yes)
```

Power analysis
```{r}
h <- pwr::ES.h(p_it_yes, p_acad_yes)

pwr::pwr.2p2n.test(
  h = h,
  n1 = n_acad,
  sig.level = 0.05,
  power = 0.8,
  alternative = "less"
)
```

Great. We only need 19 IT people, but we have 33.

Now let's proceed with our hypothesis test.
```{r}
pairwise_z_test_lessthan(
  it_acad,
  outcome_col = "Job",
  predictor_col = "Role",
  group1 = "IT",
  group2 = "Academic"
)
```

Good! A pairwise z-test for proportions says the difference in "success rates" (proportion of yes answers) for IT (21% said yes) and academics (50% said yes) for the "Job" motivation is indeed significant.

I know this notebook should be for stats, and the other should be for plots, but it's just easier right now to make a plot here using the data structures I already have, instead of reconstructing them again in the other script.
```{r, fig.width=9, fig.height=9}
to_plot <- data.frame(proportion_yes=c(p_acad_yes, p_it_yes), role=c("Academic", "IT"))

basic_bar_chart(to_plot,
  x_var = "role",
  y_var = "proportion_yes",
  title = "Percent of Respondents who said\n'Developing open source products is part of my job'",
  show_bar_labels = FALSE,
  show_ticks_y = FALSE,
  show_axis_title_y = FALSE,
  show_axis_title_x = FALSE,
  show_grid = TRUE,
  percent = TRUE
)
```

Save the plot if desired.
```{r}
save_plot("acad_it_simple.tiff", 9, 9)
```

# Session Info
```{r}
sessionInfo()
```