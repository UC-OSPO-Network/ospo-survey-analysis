---
title: "Project sizes: statistics"
---

# Overview
This notebook explores Q5: "How frequently have you contributed to projects of the following size?". I am following up on some plots and counts from sizes_plots.qmd with statistics.

# Import packages and utilities
```{r}
project_root <- here::here() # requires that you be somewhere in the
# project directory (not above it)
# packages
suppressMessages(source(file.path(project_root, "scripts/packages.R")))
# functions and objects used across scripts
suppressMessages(source(file.path(project_root, "scripts/utils.R")))
```

# Load data
```{r}
sizes_raw <- load_qualtrics_data("clean_data/project_size_Q5.tsv")
other_quant <- load_qualtrics_data("clean_data/other_quant.tsv")
```

# Wrangle data

```{r}
sizes_job <- cbind(sizes_raw, other_quant$job_category)
# Rename column
names(sizes_job)[ncol(sizes_job)] <- "job_category"
# Filter out people who didn't answer either question
sizes_job <- exclude_empty_rows(sizes_job, strict = TRUE)
sizes_job$participantID <- seq(nrow(sizes_job))
```

```{r}
sizes_job_long <- sizes_job %>%
  pivot_longer(
    cols = -c(job_category, participantID),
    names_to = "size",
    values_to = "frequency"
  )

head(sizes_job_long)
```

```{r}
# three way cross tabs (xtabs) and flatten the table
# code from: https://ladal.edu.au/tutorials/regression/regression.html
ftable(xtabs(~ job_category + size + frequency, data = sizes_job_long))
```

## Create different job category labels
Let's fold in the smaller job categories, like we did with the other regressions. Acutally, let's try it two ways: first, with 4 groups (nr staff, students, postdocs, and faculty), and then with two: non-research staff vs. academics. We'll see which model looks better.

```{r}
combined4 <- sizes_job_long %>%
  mutate(
    job_category = recode(
      job_category,
      "Post-Doc" = "Postdocs and Staff Researchers",
      "Other research staff" = "Postdocs and Staff Researchers"
    )
  )

combined4 <- combined4 %>%
  mutate(
    job_category = recode(
      job_category,
      "Grad Student" = "Students",
      "Undergraduate" = "Students"
    )
  )
```

```{r}
combined2 <- sizes_job_long %>%
  mutate(
    job_category = recode(
      job_category,
      "Grad Student" = "Academic",
      "Undergraduate" = "Academic",
      "Other research staff" = "Academic",
      "Post-Doc" = "Academic",
      "Faculty" = "Academic"
    )
  )

# example
combined2
```

Reorder factor levels.
```{r}
ordered_sizes <- c(
  "Small",
  "Medium",
  "Large"
)

ordered_freqs <- c(
  "Never",
  "Relatively infrequently",
  "Occasionally",
  "Relatively frequently"
)

ordered_jobs4 <- c(
  "Students",
  "Postdocs and Staff Researchers",
  "Faculty",
  "Non-research Staff"
)

ordered_jobs2 <- c(
  "Academic",
  "Non-research Staff"
)

combined4$size <- factor(combined4$size, levels = ordered_sizes)
combined4$frequency <- factor(combined4$frequency, levels = ordered_freqs)
combined4$job_category <- factor(combined4$job_category, levels = ordered_jobs4)

combined2$size <- factor(combined2$size, levels = ordered_sizes)
combined2$frequency <- factor(combined2$frequency, levels = ordered_freqs)
combined2$job_category <- factor(combined2$job_category, levels = ordered_jobs2)
```

# Model selection part 1: comparing non-mixed models

To start with, I'm going to create and compare a variety of non-mixed models to see which one looks best. clm(), which is for non-mixed models, has a couple of diagnostic capabilities that clmm() (mixed models) does not. Once I've chosen a non-mixed model that looks good, I'll add in the random effects term.

Here are the models I want to inspect:\
freq ~ size\
freq ~ combined2 + size\
freq ~ combined2 * size\
freq ~ combined4 + size\
freq ~ combined4 * size\

## Model 1: no job data

```{r}
# Since we're ignoring the job_category column, 
# it doesn't matter which data frame we use
fit1 <- ordinal::clm(frequency ~ size,                 
            data = combined2, link = "logit", Hess = TRUE)
```

## Model 2: 2 job categories, no interaction

```{r}
fit2 <- ordinal::clm(frequency ~ job_category + size,                 
            data = combined2, link = "logit", Hess = TRUE)
```

## Model 3: 4 job categories, no interaction

```{r}
fit3 <- ordinal::clm(frequency ~ job_category + size,                 
            data = combined4, link = "logit", Hess = TRUE)
```

## Model 4: 2 job categories, with interaction

```{r}
fit4 <- ordinal::clm(frequency ~ job_category * size,                 
            data = combined2, link = "logit", Hess = TRUE)
```

## Model 5: 4 job categories, with interaction

```{r}
fit5 <- ordinal::clm(frequency ~ job_category * size,                 
            data = combined4, link = "logit", Hess = TRUE)
```

# Goodness-of-fit

## AICs

```{r}
models <- list(
  "fit1"=fit1,
  "fit2"=fit2,
  "fit3"=fit3,
  "fit4"=fit4,
  "fit5"=fit5
)
```

First, let's get a general sense of goodness-of-fit by looking at the AICs. You're not supposed to compare AICs for models fit to different data sets, but since I've only changed the job_category labels, not the observations or the number of observations, I think this is ok.

```{r}
sapply(models, function(x) round(stats::AIC(x)))
```

AICs are very similar across the board. The last two models look a teensy bit better.

## "Condition number of the Hessian"

Let's check the condition number of the Hessian. I don't really understand what this is, but the clmm2 tutorial says that high numbers, say larger than say 10^4 or 10^6, indicate poor fit.
```{r}
sapply(models, function(x) 
summary(x)$info["cond.H"]
)
```

All look fine.

## ANOVAs

Let's use some anovas to compare nested models.
```{r}
stats::anova(fit1, fit2)
```

Hmm. Including two job categories (no interaction) is just barely significant.

```{r}
stats::anova(fit1, fit3)
```

Including four job categories (no interaction) is slightly more significant. Let's proceed with fit3 as the one to beat. We'll compare it to fit5, which uses the same job labels.

```{r}
stats::anova(fit3, fit5)
```

Yes, inclusion of the interaction term is helpful.

## Nominal and scale tests for clm

https://www.rdocumentation.org/packages/ordinal/versions/2023.12-4.1/topics/nominal_test

nominal_test(), as I understand it, tests for violations of the proportional odds assumption, which is the assumption that the effect of the explanatory variables are the same across all levels of the outcome variable (remember, we're assuming the outcome categories are cut-offs of an underlying continuous variable). It does ANOVA/LRT on models where a predictor is allowed to have different effects on the different factor levels (cut-off regions), and tests if a version of the model where this assumption is relaxed is a significantly better fit than the one where the assumption is required.

scale_test() does the same sort of thing, but instead of testing for non-proportional odds, it's testing for heteroskedasticity.

```{r}
nominal_test(fit5)
scale_test(fit5)
```

Hmm. Looks like the interaction term violates the non-proportional odds assumption. In other words, the effect of the interaction varies at different frequency levels. No bueno. But the model is improved by the interaction term, according to the LRTs, so we don't want to get rid of it. I will proceed cautiously, and maybe just report it? I really don't want to get into the weeds with this.

# Mixed models

## Model 6: size as fixed effect only

```{r}
fit6 <- ordinal::clmm(frequency ~ job_category * size +
          (1 | participantID),               
          data = combined4, link = "logit", Hess = TRUE)
```

## Model 7: size as both a fixed and random effect

```{r}
fit7 <- ordinal::clmm(frequency ~ job_category * size +
          (1 + size | participantID),               
          data = combined4, link = "logit", Hess = TRUE)
```

Hm. I think the 4-job-category data are too sparse for this model to converge. I'm kind of curious about the 2-job label.

## Model 8: size as fixed + random, 2 job cats

```{r}
fit8 <- ordinal::clmm(frequency ~ job_category * size +
          (1 + size | participantID),               
          data = combined2, link = "logit", Hess = TRUE)
```

Hm. This one is having the same problem. I guess we'll proceed with the one that converged.

# Goodness-of-fit: mixed

```{r}
stats::anova(fit5, fit6)
```

It appears that the model is very much improved by including the participant ID as a random effect (each participant has their own intercept ("baseline")). Since clmm() doesn't support relaxing the non-proportional odds requirement with nominal=, and since I really don't want to switch to a new package/methodology that does, I'm just going to report this and move forward with fit6.

# Hypothesis testing (emmeans)
```{r}
summary(fit6)
```

That's a lot of parameters to interpret. emmeans to the rescue.

I'm not going to attempt to average the results across job category, so there's no weighting scheme needed.

```{r}
emm <- emmeans(fit6, ~ size * job_category, mode = "mean.class")
```

```{r}
summary(emm) %>% 
  arrange(desc(mean.class))
```

Hmm. Mildly interesting. As suggested by the exploratory plots, the results are fairly clean for small and large projects, but more muddled for medium projects.

```{r}
by_job <- summary(
  pairs(emm, by = "job_category"),
  infer = TRUE # infer CIs
)
by_job
```

Wow, okay, still a lot of parameters to interpret. 

After staring at this for a while, I think the main conclusion is that everyone is more likely to contribute to small projects than large projects.

```{r}
by_size <- summary(
  pairs(emm, by = "size"),
  infer = TRUE # infer CIs
)
by_size
```

After staring at this for a while, I think we can conclude that academics contribute to large projects less frequently than non-research staff, but there is very little evidence to support the reverse--that academics are more likely to contribute to small projects than NR staff. (Only true for postdocs and staff researchers.)

So, w.r.t. conclusion #2, I think we've mitigated the danger of the non-proportional odds violation by just looking at a single level, i.e. Large projects for different job_categories, so it doesn't matter that the effect of the interaction is different at large vs. medium, for example, because we're only looking at large. I don't think this is true for #1, where were saying that the frequency of small is higher than the frequency of large for all groups.

# Wilcoxon test

Let's use a Wilcoxon test to confirm/deny the claim that all groups contribute to small projects more than they contribute to large projects. I'm subsetting the data to just small/large, removing medium, since I'm not making claims about contributions to medium projects. Since I'm just looking at two categories, small vs. large (for each of the 4 jobs independently), I'll use a Wilcoxon test instead of a Kruskal-Wallis test.

```{r}
# Note, no need to worry about missing data as all options were mandatory
paired <- combined4 %>%
  mutate(
    freq_score = recode(
      frequency,
      "Never" = 0L,
      "Relatively infrequently" = 1L,
      "Occasionally" = 2L,
      "Relatively frequently" = 3L
    )
  ) %>% 
    select(job_category, participantID, size, freq_score) %>%
    filter(size != "Medium") %>%
    mutate(size = forcats::fct_relevel(size, "Small", "Large")) %>%
    pivot_wider(names_from = size, values_from = freq_score) %>%
    mutate(diff = Small - Large)

paired
```


```{r}
# Wilcoxon test per job_category (one-sided: Small > Large)

wilc_results <- lapply(split(paired, paired$job_category), function(df) {
  stats::wilcox.test(
    df$Small,
    df$Large,
    paired = TRUE,
    alternative = "greater",
    conf.int = TRUE,
    conf.level = 0.95
  )
})

# Example
wilc_results[[1]]
```

Let's look at the adjusted p-values.
```{r}
job_cats <- names(split(paired, paired$job_category))
wilc_results_pvals <- sapply(seq(length(job_cats)), function(i) wilc_results[[i]]$p.value)
names(wilc_results_pvals) <- job_cats

stats::p.adjust(wilc_results_pvals, method = "holm")
```

Great. This gives us more confidence in claiming that all groups contribute to small projects more than large projects.

## Wilcoxon test: focusing on large projects

Well, since we're here, we might as well also do a wilcoxon test our other finding that non-research staff contribute to large projects more frequently than academics do. These data are not paired, obviously, since we don't have repeated measures per person for just large projects. Now we're comparing between job categories instead of between project sizes.

```{r}
large_projs <- combined4 %>%
  mutate(
    freq_score = recode(
      frequency,
      "Never" = 0L,
      "Relatively infrequently" = 1L,
      "Occasionally" = 2L,
      "Relatively frequently" = 3L
    )
  ) %>%
  filter(size == "Large") %>%
  mutate(
    group = if_else(
      job_category == "Non-research Staff",
      "nrstaff",
      "academic"
    )
  ) %>%
  select(group, freq_score)

large_projs$group <- factor(large_projs$group, levels = c("nrstaff", "academic"))
```

```{r}
# one-sided: NRS > Academics
wilc_results2 <- wilcox.test(
  freq_score ~ group,
  data = large_projs,
  alternative = "greater",
  conf.int = TRUE
)

wilc_results2
```

```{r}
sessionInfo()
```